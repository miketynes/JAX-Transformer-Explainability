{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b98618fc-0e16-4442-b598-82176c5e1bd8",
   "metadata": {},
   "source": [
    "# Trying to load Torch BERT weights into Jax\n",
    "\n",
    "We'd like to load the fine-tuned weights from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e88c8-5e61-4776-96a7-9979aedff006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0942b-7850-4388-88a0-cce26921f1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fcd3216-3696-4259-bf19-3a53e44e6711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/miniconda3/envs/TransformerExplainability/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3143a3-f773-4dbe-a027-b095190643c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, FlaxBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611dd8c4-4a68-49c1-affc-d39f704d861a",
   "metadata": {},
   "source": [
    "## Load Torch Fine-tuned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c1b61-7e0f-494c-8a32-d76fa8f468a8",
   "metadata": {},
   "source": [
    "They intialize their model like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e221f9-f805-47dc-a1e2-fe3d523465a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_theirs = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e445596d-4ba8-4bfe-80f3-dbdf1f420e40",
   "metadata": {},
   "source": [
    "And then read in this state dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3e82a8-ee3a-4a65-b58b-3e4759015dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('../data/classifier/classifier.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b007ab09-a80b-4f38-bcf5-244988d9fc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_theirs.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb825c-8ccb-47de-91e8-f70570193b15",
   "metadata": {},
   "source": [
    "## Trying to load the state dict into JAX\n",
    "\n",
    "There is a [github dicussion](https://github.com/google/flax/discussions/927) from around two years ago that indicates that this is possible. But I am afraid that the details have changed a bit since then, and I can't find anything more recent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3fd765a-b2df-4e54-9148-6ecfec068015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing FlaxBertForSequenceClassification: {('cls', 'predictions', 'transform', 'dense', 'kernel'), ('cls', 'predictions', 'transform', 'dense', 'bias'), ('cls', 'predictions', 'transform', 'LayerNorm', 'scale'), ('cls', 'predictions', 'bias'), ('cls', 'predictions', 'transform', 'LayerNorm', 'bias')}\n",
      "- This IS expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: {('classifier', 'bias'), ('bert', 'pooler', 'dense', 'kernel'), ('classifier', 'kernel'), ('bert', 'pooler', 'dense', 'bias')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# instantiate the complimentary Flax BERT pretrained model\n",
    "bert_ours = FlaxBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09050705-8288-4b57-9c9c-b582c89f9d9a",
   "metadata": {},
   "source": [
    "### Attempt #1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e48285-7887-462b-b3c2-7e1c51001280",
   "metadata": {},
   "source": [
    "First I try a slightly adapted version of the method from the first link from the above github discussion https://github.com/nikitakit/flax_bert/blob/master/import_weights.py. It appears to map the torch parameter keywords to their jax counterparts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878e742-d663-4e6a-9b6a-1930942d0bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975e8e2f-af3f-4577-bea8-650b4c553e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params_from_hf(pt_params, hidden_size, num_attention_heads):\n",
    "    jax_params = {}\n",
    "    # mapping between HuggingFace PyTorch BERT and JAX model\n",
    "    pt_key_to_jax_key = [\n",
    "        # Output heads\n",
    "        ('cls.seq_relationship', 'classification'),\n",
    "        ('cls.predictions.transform.LayerNorm', 'predictions_transform_layernorm'),\n",
    "        ('cls.predictions.transform.dense', 'predictions_transform_dense'),\n",
    "        ('cls.predictions.bias', 'predictions_output.bias'),\n",
    "        ('cls.predictions.decoder.weight', 'UNUSED'),\n",
    "        ('cls.predictions.decoder.bias', 'UNUSED'),\n",
    "        # Embeddings\n",
    "        ('embeddings.position_ids', 'UNUSED'),\n",
    "        ('embeddings.word_embeddings.weight', 'word_embeddings.embedding'),\n",
    "        ('embeddings.token_type_embeddings.weight', 'type_embeddings.embedding'),\n",
    "        ('embeddings.position_embeddings.weight', 'position_embeddings.embedding'),\n",
    "        ('embeddings.LayerNorm', 'embeddings_layer_norm'),\n",
    "        # Pooler\n",
    "        ('pooler.dense.', 'pooler.'),\n",
    "        # Layers\n",
    "        ('bert.encoder.layer.', 'bert.encoder_layer_'),\n",
    "        # ('bert/encoder/layer_', 'bert/encoder_layer_'),\n",
    "        ('attention.self', 'self_attention.attn'),\n",
    "        ('attention.output.dense', 'self_attention.attn.output'),\n",
    "        ('attention.output.LayerNorm', 'self_attention_layer_norm'),\n",
    "        ('output.LayerNorm', 'output_layer_norm'),\n",
    "        ('intermediate.dense', 'feed_forward.intermediate'),\n",
    "        ('output.dense', 'feed_forward.output'),\n",
    "        # Parameter names\n",
    "        ('weight', 'kernel'),\n",
    "        ('beta', 'bias'),\n",
    "        ('gamma', 'scale'),\n",
    "        ('layer_norm.kernel', 'layer_norm.scale'),\n",
    "        ('layernorm.kernel', 'layernorm.scale'),\n",
    "        ]\n",
    "    pt_keys_to_transpose = (\n",
    "            \"dense.weight\",\n",
    "            \"attention.self.query\",\n",
    "            \"attention.self.key\",\n",
    "            \"attention.self.value\"\n",
    "            )\n",
    "    for pt_key, val in pt_params.items():\n",
    "        jax_key = pt_key\n",
    "        for pt_name, jax_name in pt_key_to_jax_key:\n",
    "            jax_key = jax_key.replace(pt_name, jax_name)\n",
    "\n",
    "        if 'UNUSED' in jax_key:\n",
    "                continue\n",
    "\n",
    "        if any([x in pt_key for x in pt_keys_to_transpose]):\n",
    "                val = val.T\n",
    "        val = np.asarray(val)\n",
    "\n",
    "        # Reshape kernels if necessary\n",
    "        reshape_params = ['key', 'query', 'value']\n",
    "        for key in reshape_params:\n",
    "            if f'self_attention.attn.{key}.kernel' in jax_key:\n",
    "                val = np.swapaxes(\n",
    "                        val.reshape((hidden_size, num_attention_heads, -1)), 0, 1)\n",
    "            elif f'self_attention.attn.{key}.bias' in jax_key:\n",
    "                val = val.reshape((num_attention_heads, -1))\n",
    "        if 'self_attention.attn.output.kernel' in jax_key:\n",
    "            val = val.reshape((num_attention_heads, -1, hidden_size))\n",
    "        elif 'self_attention.attn.output.bias' in jax_key:\n",
    "            # The multihead attention implementation we use creates a bias vector for\n",
    "            # each head, even though this is highly redundant.\n",
    "            val = np.stack(\n",
    "                    [val] + [np.zeros_like(val)] * (num_attention_heads - 1), axis=0)\n",
    "\n",
    "        jax_params[jax_key] = val\n",
    "\n",
    "    # jax position embedding kernel has additional dimension\n",
    "    pos_embedding = jax_params[\n",
    "            'bert.position_embeddings.embedding']\n",
    "    jax_params[\n",
    "            'bert.position_embeddings.embedding'] = pos_embedding[\n",
    "                    np.newaxis, ...]\n",
    "\n",
    "    # this layer doesn't have parameters, but key is required to be present\n",
    "    jax_params['GatherIndexes_0'] = {}\n",
    "\n",
    "    # convert flat param dict into nested dict using `/` as delimeter\n",
    "    outer_dict = {}\n",
    "    for key, val in jax_params.items():\n",
    "        tokens = key.split('.')\n",
    "        inner_dict = outer_dict\n",
    "        # each token except the very last should add a layer to the nested dict\n",
    "        for token in tokens[:-1]:\n",
    "            if token not in inner_dict:\n",
    "                inner_dict[token] = {}\n",
    "            inner_dict = inner_dict[token]\n",
    "        inner_dict[tokens[-1]] = val\n",
    "\n",
    "    if 'global_step' in outer_dict:\n",
    "        del outer_dict['global_step']\n",
    "\n",
    "    return outer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c85d33e0-c682-44e1-bc35-50f58f16e11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_612/1919259538.py:51: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  val = val.T\n"
     ]
    }
   ],
   "source": [
    "#their_params_fl = pt2fl(bert_theirs)\n",
    "their_params_fl = load_params_from_hf(state_dict, hidden_size=768, num_attention_heads=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b93662-cd47-4533-932c-ccc83474a466",
   "metadata": {},
   "source": [
    "(The above seems like an ignorable warning)\n",
    "\n",
    "The `FlaxBertForSequenceClassification.params` property acts as a setter, so it will complain if the new value isnt formatted correctly. So it seems that the above method does not produce a valid jax state dict. Maybe I can transform it so its correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66944ce-bf84-49f3-bba5-f24191a61496",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some parameters are missing. Make sure that `params` include the following parameters {('bert', 'encoder', 'layer', '10', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '2', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '3', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '7', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '1', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '5', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '7', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '9', 'attention', 'self', 'query', 'kernel'), ('bert', 'embeddings', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '10', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '5', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '5', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '2', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '8', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '3', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '0', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '6', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '4', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '0', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '3', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '9', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '7', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '8', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '6', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '6', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '5', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '11', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '2', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '1', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '1', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '11', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '1', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '7', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '9', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '5', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '9', 'attention', 'self', 'key', 'kernel'), ('bert', 'pooler', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '1', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '0', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '4', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '8', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '0', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '6', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '11', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '3', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '5', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '3', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '9', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '2', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '9', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '11', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '9', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '11', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '3', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '7', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '5', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '9', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '10', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '6', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '0', 'attention', 'output', 'dense', 'bias'), ('bert', 'embeddings', 'word_embeddings', 'embedding'), ('bert', 'encoder', 'layer', '10', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '2', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '4', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '8', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '2', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '6', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '4', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '9', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '5', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '11', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '7', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '0', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '1', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '1', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '0', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '1', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '1', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '6', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '2', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '7', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '9', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '3', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '4', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '6', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '2', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '11', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '5', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '11', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '8', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '5', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '9', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '2', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '4', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '7', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '6', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '1', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '11', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '5', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '9', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'embeddings', 'token_type_embeddings', 'embedding'), ('bert', 'encoder', 'layer', '0', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '2', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '5', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '8', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '8', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '3', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '4', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '1', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '10', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '6', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '11', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '5', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '3', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '6', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '6', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '6', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '11', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '7', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '11', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '4', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '7', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '7', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '8', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '1', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '10', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '2', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '8', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '3', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '3', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '9', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '8', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '8', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '7', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '10', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '1', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '0', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '11', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '1', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'embeddings', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '5', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '4', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '11', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '0', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '10', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '2', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '0', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '9', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '7', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '8', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '8', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '2', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '3', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '3', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '4', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '0', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '4', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '2', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '3', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '5', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '0', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '6', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '10', 'output', 'LayerNorm', 'bias'), ('bert', 'embeddings', 'position_embeddings', 'embedding'), ('bert', 'encoder', 'layer', '8', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '2', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '6', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '5', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '4', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '7', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '7', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '11', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '4', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '4', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '9', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'self', 'value', 'kernel'), ('bert', 'pooler', 'dense', 'bias'), ('bert', 'encoder', 'layer', '7', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '1', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '3', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '0', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '9', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '4', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '1', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '8', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '8', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '0', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '6', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '0', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '3', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '4', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '11', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '2', 'attention', 'self', 'key', 'bias')}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbert_ours\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m \u001b[38;5;241m=\u001b[39m their_params_fl\n",
      "File \u001b[0;32m~/miniconda3/envs/TransformerExplainability/lib/python3.10/site-packages/transformers/modeling_flax_utils.py:288\u001b[0m, in \u001b[0;36mFlaxPreTrainedModel.params\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    286\u001b[0m param_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(flatten_dict(params)\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired_params \u001b[38;5;241m-\u001b[39m param_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome parameters are missing. Make sure that `params` include the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired_params \u001b[38;5;241m-\u001b[39m param_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m params\n",
      "\u001b[0;31mValueError\u001b[0m: Some parameters are missing. Make sure that `params` include the following parameters {('bert', 'encoder', 'layer', '10', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '2', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '3', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '7', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '1', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '5', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '7', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '9', 'attention', 'self', 'query', 'kernel'), ('bert', 'embeddings', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '10', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '5', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '5', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '2', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '8', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '3', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '0', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '6', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '4', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '0', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '3', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '9', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '7', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '8', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '6', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '6', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '5', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '11', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '2', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '1', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '1', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '11', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '1', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '7', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '9', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '5', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '9', 'attention', 'self', 'key', 'kernel'), ('bert', 'pooler', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '1', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '0', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '4', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '8', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '0', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '6', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '11', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '3', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '5', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '3', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '9', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '2', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '9', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '11', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '9', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '11', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '3', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '7', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '5', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '9', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '10', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '6', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '0', 'attention', 'output', 'dense', 'bias'), ('bert', 'embeddings', 'word_embeddings', 'embedding'), ('bert', 'encoder', 'layer', '10', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '2', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '4', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '8', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '2', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '6', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '4', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '9', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '5', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '11', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '7', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '0', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '1', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '1', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '0', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '1', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '1', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '6', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '2', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '7', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '9', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '3', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '4', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '6', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '2', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '11', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '5', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '11', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '8', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '5', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '9', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '2', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '4', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '7', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '6', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '1', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '11', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '5', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '9', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'embeddings', 'token_type_embeddings', 'embedding'), ('bert', 'encoder', 'layer', '0', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '2', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '5', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '8', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '8', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '3', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '4', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '1', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '10', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '6', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '11', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '5', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '3', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '6', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '6', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '6', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '11', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '7', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '11', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '4', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '7', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '7', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '8', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '1', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '10', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '2', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '8', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '3', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '3', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '9', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '8', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '8', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '7', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '10', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '1', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '0', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '11', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '1', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'embeddings', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '5', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '4', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '11', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '0', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '10', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '2', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '0', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '9', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '7', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '8', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '8', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '2', 'attention', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '3', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '3', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '4', 'attention', 'self', 'value', 'bias'), ('bert', 'encoder', 'layer', '0', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '4', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '2', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '3', 'attention', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '5', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '0', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '6', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '10', 'output', 'LayerNorm', 'bias'), ('bert', 'embeddings', 'position_embeddings', 'embedding'), ('bert', 'encoder', 'layer', '8', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '2', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '6', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '5', 'attention', 'self', 'query', 'bias'), ('bert', 'encoder', 'layer', '4', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '7', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '7', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '11', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '4', 'output', 'LayerNorm', 'scale'), ('bert', 'encoder', 'layer', '4', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '9', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '10', 'attention', 'self', 'value', 'kernel'), ('bert', 'pooler', 'dense', 'bias'), ('bert', 'encoder', 'layer', '7', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '1', 'attention', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '3', 'attention', 'self', 'key', 'kernel'), ('bert', 'encoder', 'layer', '0', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '9', 'output', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '4', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '1', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '8', 'attention', 'output', 'LayerNorm', 'bias'), ('bert', 'encoder', 'layer', '8', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '0', 'attention', 'self', 'key', 'bias'), ('bert', 'encoder', 'layer', '6', 'intermediate', 'dense', 'bias'), ('bert', 'encoder', 'layer', '0', 'attention', 'self', 'value', 'kernel'), ('bert', 'encoder', 'layer', '3', 'output', 'dense', 'bias'), ('bert', 'encoder', 'layer', '4', 'intermediate', 'dense', 'kernel'), ('bert', 'encoder', 'layer', '11', 'attention', 'self', 'query', 'kernel'), ('bert', 'encoder', 'layer', '2', 'attention', 'self', 'key', 'bias')}"
     ]
    }
   ],
   "source": [
    "bert_ours.params = their_params_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b1cfc-cb24-4b3d-a933-9c764d1f9c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55135cc8-0dc4-48bb-a9bc-84affa42872f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c2abf6d-651d-4dbe-87f6-ae2747041ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.traverse_util import flatten_dict, unflatten_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23ed510b-df58-49ec-a587-76a159f0dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_keys = bert_ours.required_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bb77b5b-2f57-4062-bfef-68cb3fb22d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_keys_transformed = set(flatten_dict(their_params_fl).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2273eedb-d7ab-4a01-8bfc-a48cae060c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_flattned_keys(keys): \n",
    "    keys = list(keys)\n",
    "    for i, key in enumerate(keys): \n",
    "        new_key = list(key)\n",
    "        if 'encoder_layer_' in key[1]: \n",
    "            new_key = [new_key[0]] + new_key[1].split('_') + new_key[2:]\n",
    "        if new_key[1] == 'embeddings_layer_norm': \n",
    "            new_key = [new_key[0]] + ['embeddings', 'LayerNorm'] + [new_key[-1]]\n",
    "        if new_key[1] in ['position_embeddings', \n",
    "                          'word_embeddings', \n",
    "                          'type_embeddings']: \n",
    "            if new_key[1] == 'type_embeddings': \n",
    "                new_key[1] = 'token_type_embeddings'\n",
    "            new_key = [new_key[0]] + ['embeddings'] + new_key[1:]\n",
    "        if new_key[1] == 'pooler': \n",
    "            new_key.insert(-1, 'dense')\n",
    "        if len(key) > 3:\n",
    "            if new_key[4] == 'feed_forward': \n",
    "                new_key[4] = 'dense'\n",
    "            if new_key[4] == 'self_attention': \n",
    "                new_key[4] = 'attention'\n",
    "                new_key[5] = 'self'\n",
    "                if new_key[6] == 'output':\n",
    "                    new_key[5] = 'output'\n",
    "                    new_key[6] = 'dense'\n",
    "            if new_key[-3:-1] == ['dense', 'intermediate']: \n",
    "                new_key[-3] = 'intermediate'\n",
    "                new_key[-2] = 'dense'\n",
    "            if new_key[-2] == 'output_layer_norm': \n",
    "                new_key = new_key[:-2] + ['output', 'LayerNorm'] + new_key[-1:]\n",
    "            if new_key[-2] == 'self_attention_layer_norm': \n",
    "                new_key = new_key[:-2] + ['attention', 'output', 'LayerNorm'] + new_key[-1:]\n",
    "        if len(new_key) == 7: \n",
    "            if '.'.join(new_key[:3]) == 'bert.encoder.layer' and \\\n",
    "            '.'.join(new_key[-3:]) in ['dense.output.bias', 'dense.output.kernel']: \n",
    "                new_key[-3] = 'output'\n",
    "                new_key[-2] = 'dense'\n",
    "        keys[i] = tuple(new_key)\n",
    "    return set(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f88e0c3f-d232-4657-8064-941485655426",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_keys_cleaned = postprocess_flattned_keys(flattened_keys_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b666605-0c1a-46d7-a691-f3c7a09fcabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flattened_keys_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06eb764c-d6c0-4a8e-8234-d91d4bfaa103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_keys - flattened_keys_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75544a30-04b5-4c91-8f31-14263ba63f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_keys_cleaned - required_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75124a-a9ff-4808-8edf-daf23d7d3121",
   "metadata": {},
   "source": [
    "Aww yeah, but now i have to use these keys to line up the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd0d115f-fe5d-4925-ba98-2ea883f9d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_state_dict(processed_state_dict):\n",
    "    state_dict_flat = flatten_dict(processed_state_dict)\n",
    "    clean_state_dict = {}\n",
    "    for key in state_dict_flat.keys(): \n",
    "        new_key = list(key)\n",
    "        if 'encoder_layer_' in key[1]: \n",
    "            new_key = [new_key[0]] + new_key[1].split('_') + new_key[2:]\n",
    "        if new_key[1] == 'embeddings_layer_norm': \n",
    "            new_key = [new_key[0]] + ['embeddings', 'LayerNorm'] + [new_key[-1]]\n",
    "        if new_key[1] in ['position_embeddings', \n",
    "                          'word_embeddings', \n",
    "                          'type_embeddings']: \n",
    "            if new_key[1] == 'type_embeddings': \n",
    "                new_key[1] = 'token_type_embeddings'\n",
    "            new_key = [new_key[0]] + ['embeddings'] + new_key[1:]\n",
    "        if new_key[1] == 'pooler': \n",
    "            new_key.insert(-1, 'dense')\n",
    "        if len(key) > 3:\n",
    "            if new_key[4] == 'feed_forward': \n",
    "                new_key[4] = 'dense'\n",
    "            if new_key[4] == 'self_attention': \n",
    "                new_key[4] = 'attention'\n",
    "                new_key[5] = 'self'\n",
    "                if new_key[6] == 'output':\n",
    "                    new_key[5] = 'output'\n",
    "                    new_key[6] = 'dense'\n",
    "            if new_key[-3:-1] == ['dense', 'intermediate']: \n",
    "                new_key[-3] = 'intermediate'\n",
    "                new_key[-2] = 'dense'\n",
    "            if new_key[-2] == 'output_layer_norm': \n",
    "                new_key = new_key[:-2] + ['output', 'LayerNorm'] + new_key[-1:]\n",
    "            if new_key[-2] == 'self_attention_layer_norm': \n",
    "                new_key = new_key[:-2] + ['attention', 'output', 'LayerNorm'] + new_key[-1:]\n",
    "        if len(new_key) == 7: \n",
    "            if '.'.join(new_key[:3]) == 'bert.encoder.layer' and \\\n",
    "            '.'.join(new_key[-3:]) in ['dense.output.bias', 'dense.output.kernel']: \n",
    "                new_key[-3] = 'output'\n",
    "                new_key[-2] = 'dense'\n",
    "        clean_state_dict[tuple(new_key)] = state_dict_flat[key]\n",
    "    return unflatten_dict(clean_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bee5a9c-f460-4314-b7b3-5b86212ecb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sd = clean_state_dict(their_params_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1de01413-d337-4e14-8068-2fafec91f73f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_ours.params = new_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f70bb8ed-1361-4dbf-af6b-b5a334055069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_flax_bert.FlaxBertForSequenceClassification at 0x7f02b4db6fe0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceebd3a-4bc0-4a08-878e-9d59924d4742",
   "metadata": {},
   "source": [
    "Well, hopefully this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654ddc0-c398-49a1-8ac3-438f0b2a9d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
