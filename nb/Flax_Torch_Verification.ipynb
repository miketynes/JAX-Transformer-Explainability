{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Modules needed to be verified\n",
    "\n",
    "\n",
    "## Basic Building Blocks (bb)\n",
    "all the components in the layers file\n",
    "\n",
    "- FlaxBertForSequenceClassification\n",
    "    - FlaxBertForSequenceClassificationModule\n",
    "        - FlaxBertModule\n",
    "            - FlaxBertEmbeddings\n",
    "                - Add\n",
    "                - LayerNorm\n",
    "                - Dropout\n",
    "            - FlaxBertEncoder\n",
    "                - FlaxBertLayerCollection\n",
    "                    - FlaxBertLayer\n",
    "                        - FlaxBertAttention\n",
    "                            - FlaxBertSelfAttention(bb only)\n",
    "                            - ~~FlaxBertSelfOutput(bb only)~~ ?\n",
    "                        - ~~FlaxBertIntermediate(bb only)~~\n",
    "                        - FlaxBertOutput(bb only)\n",
    "                    - FlaxBertCheckpointLayer(cond.)\n",
    "            - FlaxBertPooler(bb only)\n",
    "        - Dropout\n",
    "        - Dense"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Building Block verification.\n",
    "- FlaxBertSelfAttention vs.     BertSelfAttention\n",
    "- FlaxBertSelfOutput    vs.     BertSelfOutput\n",
    "- FlaxBertIntermediate  vs.     BertIntermediate\n",
    "- FlaxBertOutput        vs.     BertOutput\n",
    "- FlaxBertPooler        vs.     BertPooler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 23:04:18.167464: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-01 23:04:18.167502: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-01 23:04:18.167505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from clu import parameter_overview\n",
    "import flax.linen as fnn\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from bert_torch.BERT import *\n",
    "from bert.modeling_flax_bert import *\n",
    "import bert.modeling_flax_bert as layers\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "import bert_torch.layers as tl\n",
    "import bert.bert_explainability_layers as fl\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "configuration = BertConfig()\n",
    "print(configuration.add_cross_attention)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': DeviceArray([[ 101, 7592, 2088,  999,  102]], dtype=int32), 'token_type_ids': DeviceArray([[0, 0, 0, 0, 0]], dtype=int32), 'attention_mask': DeviceArray([[1, 1, 1, 1, 1]], dtype=int32), 'position_ids': DeviceArray([0], dtype=int32)} {'input_ids': tensor([[ 101, 7592, 2088,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'position_ids': tensor([0])}\n"
     ]
    }
   ],
   "source": [
    "text_batch = [\"Hello world!\"]\n",
    "fl_inputs = tokenizer(text_batch)\n",
    "# print(fl_inputs.__class__)\n",
    "# print(dict(inputs).__class__)\n",
    "\n",
    "for k in fl_inputs:\n",
    "    fl_inputs[k] = jnp.array(fl_inputs[k])\n",
    "fl_inputs['position_ids'] = jnp.arange(fl_inputs['input_ids'].shape[0])\n",
    "\n",
    "input_ids = jnp.array(fl_inputs[\"input_ids\"])\n",
    "token_ids = jnp.array(fl_inputs[\"token_type_ids\"])\n",
    "attention_mask = jnp.array(fl_inputs[\"attention_mask\"])\n",
    "position_ids = jnp.arange(input_ids.shape[0])\n",
    "\n",
    "pt_inputs = tokenizer(text_batch, return_tensors='pt')\n",
    "pt_inputs['position_ids'] = torch.tensor(np.arange(pt_inputs['input_ids'].shape[0]))\n",
    "pt_input_ids = pt_inputs[\"input_ids\"]\n",
    "pt_token_ids = pt_inputs[\"token_type_ids\"]\n",
    "pt_attention_mask = pt_inputs[\"attention_mask\"]\n",
    "pt_position_ids = torch.tensor(np.arange(pt_input_ids.shape[0]))\n",
    "\n",
    "print(fl_inputs, pt_inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# pt_layer = BertEmbeddings(configuration).eval()\n",
    "# fl_layer = FlaxBertEmbeddings(configuration)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "def nested_set(dic, keys, value, create_missing=True):\n",
    "    d = dic\n",
    "    for key in keys[:-1]:\n",
    "        if key in d:\n",
    "            d = d[key]\n",
    "        elif create_missing:\n",
    "            d = d.setdefault(key, {})\n",
    "        else:\n",
    "            return dic\n",
    "    if keys[-1] in d or create_missing:\n",
    "        d[keys[-1]] = value\n",
    "    return dic\n",
    "\n",
    "\n",
    "def pt2fl(pt: nn.Module, debug=False):\n",
    "    vf = {}\n",
    "    for k, v in pt.named_parameters():\n",
    "        keys = k.split('.')\n",
    "        if len(keys) >= 2:\n",
    "            if debug:\n",
    "                print(k, v.size())\n",
    "            if keys[-2].endswith('embeddings'):\n",
    "                keys[-1] = 'embedding'\n",
    "\n",
    "            if keys[-1] == 'weight':\n",
    "                if keys[-2] == 'LayerNorm':\n",
    "                    keys[-1] = 'scale'\n",
    "                else:\n",
    "                    keys[-1] = 'kernel'\n",
    "\n",
    "            if keys[-2] in ['dense', 'key', 'value', 'query'] and keys[-1] == 'kernel':\n",
    "                nested_set(vf, keys, value=jnp.transpose(v.detach().numpy(), (1, 0)))\n",
    "            else:\n",
    "                nested_set(vf, keys, value=v.detach().numpy())\n",
    "        else:\n",
    "            if keys[-1] == 'weight':\n",
    "                keys[-1] = 'kernel'\n",
    "                nested_set(vf, keys, value=jnp.transpose(v.detach().numpy(), (1, 0)))\n",
    "            else:\n",
    "                nested_set(vf, keys, value=v.detach().numpy())\n",
    "    params = {'params': vf}\n",
    "    if debug:\n",
    "        print('Num Prams: ', count_parameters(pt))\n",
    "        print(parameter_overview.get_parameter_overview(params))\n",
    "    return params\n",
    "\n",
    "\n",
    "def verify_module(pt:nn.Module, fl:fnn.Module, pt_kwargs=None, fl_kwargs=None, debug=False):\n",
    "    params = pt2fl(pt, debug=debug)\n",
    "\n",
    "    pt_out = pt(**pt_kwargs)\n",
    "    fl_out = fl.apply(params, **fl_kwargs)\n",
    "    # print(pt_out)\n",
    "    if isinstance(pt_out, tuple):\n",
    "        pt_out = pt_out[0]\n",
    "    if isinstance(fl_out, tuple):\n",
    "        fl_out = fl_out[0]\n",
    "    if isinstance(pt_out, BaseModelOutput):\n",
    "        pt_out = pt_out[0]\n",
    "\n",
    "    if isinstance(fl_out, FlaxBaseModelOutputWithPastAndCrossAttentions):\n",
    "        fl_out = fl_out[0]\n",
    "    if isinstance(pt_out, BaseModelOutputWithPooling):\n",
    "        pt_out = pt_out[1]\n",
    "    if isinstance(fl_out, FlaxBaseModelOutputWithPoolingAndCrossAttentions):\n",
    "        fl_out = fl_out[1]\n",
    "    print(\"Forward diff: \",np.abs(pt_out.detach().numpy() - fl_out).sum())\n",
    "\n",
    "    # print(parameter_overview.get_parameter_overview(params))\n",
    "    # print(pt, count_parameters(pt))\n",
    "\n",
    "    cam = np.random.rand(*fl_out.shape)\n",
    "    fl_cam = jnp.array(cam / cam.sum())\n",
    "    pt_cam = torch.tensor(cam / cam.sum())\n",
    "\n",
    "    fl_cams = fl.apply(params, fl_cam, **fl_kwargs, method=fl.relprop)\n",
    "    kwargs = {'alpha': 1}\n",
    "    pt_cams = pt.relprop(pt_cam, **kwargs)\n",
    "\n",
    "    print('Flax relprop:', len(fl_cams))\n",
    "    print('Pt relprop:', len(pt_cams))\n",
    "    pt_sum = None\n",
    "    fl_sum = None\n",
    "    if isinstance(fl_cams, list) or isinstance(fl_cams, tuple):\n",
    "\n",
    "        for i, c in enumerate(fl_cams):\n",
    "            print(\"Relprop size:\", pt_cams[i].size())\n",
    "            print(\"Relprop\", i, \" diff:\", np.abs(np.array(c) - pt_cams[i].detach().numpy()).sum())\n",
    "            if pt_sum:\n",
    "                pt_sum+=pt_cams[i].sum()\n",
    "            else:\n",
    "                pt_sum = pt_cams[i].sum()\n",
    "\n",
    "            if fl_sum:\n",
    "                fl_sum+=c.sum()\n",
    "            else:\n",
    "                fl_sum = c.sum()\n",
    "    else:\n",
    "        pt_sum = pt_cams.sum()\n",
    "        fl_sum = fl_cams.sum()\n",
    "        print(\"Relprop size:\", pt_cams.size())\n",
    "        print(\"Relprop diff:\", np.abs(np.array(fl_cams) - pt_cams.detach().numpy()).sum())\n",
    "    print(\"Cam sum:\", pt_sum, fl_sum)\n",
    "    return pt_out, fl_out\n",
    "\n",
    "def verify_module_args(pt:nn.Module, fl:fnn.Module, pt_kwargs=None, fl_kwargs=None, debug=False):\n",
    "    params = pt2fl(pt, debug=debug)\n",
    "\n",
    "    pt_out = pt(*pt_kwargs)\n",
    "    fl_out = fl.apply(params, *fl_kwargs)\n",
    "    # print(pt_out)\n",
    "    if isinstance(pt_out, tuple):\n",
    "        pt_out = pt_out[0]\n",
    "    if isinstance(fl_out, tuple):\n",
    "        fl_out = fl_out[0]\n",
    "    if isinstance(pt_out, BaseModelOutput):\n",
    "        pt_out = pt_out[0]\n",
    "\n",
    "    if isinstance(fl_out, FlaxBaseModelOutputWithPastAndCrossAttentions):\n",
    "        fl_out = fl_out[0]\n",
    "    if isinstance(pt_out, BaseModelOutputWithPooling):\n",
    "        pt_out = pt_out[1]\n",
    "    if isinstance(fl_out, FlaxBaseModelOutputWithPoolingAndCrossAttentions):\n",
    "        fl_out = fl_out[1]\n",
    "    print(\"Forward diff: \",np.abs(pt_out.detach().numpy() - fl_out).sum())\n",
    "    if debug:\n",
    "        # print(fl_out)\n",
    "        # print(pt_out.detach().numpy())\n",
    "        print(pt_out.detach().numpy() - fl_out)\n",
    "    # print(parameter_overview.get_parameter_overview(params))\n",
    "    # print(pt, count_parameters(pt))\n",
    "\n",
    "    cam = np.random.rand(*fl_out.shape)\n",
    "    fl_cam = jnp.array(cam / cam.sum())\n",
    "    pt_cam = torch.tensor(cam / cam.sum())\n",
    "\n",
    "    fl_cams = fl.apply(params, fl_cam, *fl_kwargs, method=fl.relprop)\n",
    "    kwargs = {'alpha': 1}\n",
    "    pt_cams = pt.relprop(pt_cam, **kwargs)\n",
    "\n",
    "    print('Flax relprop:', len(fl_cams))\n",
    "    print('Pt relprop:', len(pt_cams))\n",
    "\n",
    "    if isinstance(fl_cams, list) or isinstance(fl_cams, tuple):\n",
    "        for i, c in enumerate(fl_cams):\n",
    "            print(\"Relprop size:\", pt_cams[i].size())\n",
    "            print(\"Relprop\", i, \" diff:\", np.abs(np.array(c) - pt_cams[i].detach().numpy()).sum())\n",
    "    else:\n",
    "        print(\"Relprop size:\", pt_cams.size(), \". Cam sum:\", fl_cams.sum(), pt_cams.sum() )\n",
    "        print(\"Relprop diff:\", np.abs(np.array(fl_cams) - pt_cams.detach().numpy()).sum())\n",
    "    return pt_out, fl_out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': DeviceArray([[ 101, 7592, 2088,  999,  102]], dtype=int32), 'token_type_ids': DeviceArray([[0, 0, 0, 0, 0]], dtype=int32), 'attention_mask': DeviceArray([[1, 1, 1, 1, 1]], dtype=int32), 'position_ids': DeviceArray([0], dtype=int32)} {'input_ids': tensor([[ 101, 7592, 2088,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'position_ids': tensor([0])}\n",
      "word_embeddings.weight torch.Size([30522, 768])\n",
      "position_embeddings.weight torch.Size([512, 768])\n",
      "token_type_embeddings.weight torch.Size([2, 768])\n",
      "LayerNorm.weight torch.Size([768])\n",
      "LayerNorm.bias torch.Size([768])\n",
      "Num Prams:  23837184\n",
      "+----------------------------------------+--------------+------------+-----------+------+\n",
      "| Name                                   | Shape        | Size       | Mean      | Std  |\n",
      "+----------------------------------------+--------------+------------+-----------+------+\n",
      "| params/LayerNorm/bias                  | (768,)       | 768        | 0.0       | 0.0  |\n",
      "| params/LayerNorm/scale                 | (768,)       | 768        | 1.0       | 0.0  |\n",
      "| params/position_embeddings/embedding   | (512, 768)   | 393,216    | 0.000982  | 1.0  |\n",
      "| params/token_type_embeddings/embedding | (2, 768)     | 1,536      | -0.0501   | 1.01 |\n",
      "| params/word_embeddings/embedding       | (30522, 768) | 23,440,896 | -7.38e-05 | 1.0  |\n",
      "+----------------------------------------+--------------+------------+-----------+------+\n",
      "Total: 23,837,184\n",
      "Forward diff:  0.0001721783\n",
      "Flax relprop: 2\n",
      "Pt relprop: 2\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop 0  diff: 6.283294e-07\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop 1  diff: 5.9971546e-07\n",
      "Cam sum: tensor(1.0000, grad_fn=<AddBackward0>) 0.99999994\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Verified\"\"\"\n",
    "pt_m = BertEmbeddings(configuration).eval()\n",
    "fl_m = FlaxBertEmbeddings(configuration)\n",
    "fl_in = fl_inputs.copy()\n",
    "pt_in = pt_inputs.copy()\n",
    "print(fl_in, pt_in)\n",
    "# fl_in.pop('attention_mask', None)\n",
    "pt_in.pop('attention_mask', None)\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in, debug=True)\n",
    "\n",
    "print(pt_out.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Prams:  590592\n",
      "+---------------+------------+---------+-----------+--------+\n",
      "| Name          | Shape      | Size    | Mean      | Std    |\n",
      "+---------------+------------+---------+-----------+--------+\n",
      "| params/bias   | (768,)     | 768     | -0.000103 | 0.0201 |\n",
      "| params/kernel | (768, 768) | 589,824 | 1.83e-05  | 0.0208 |\n",
      "+---------------+------------+---------+-----------+--------+\n",
      "Total: 590,592\n",
      "Forward diff:  0.29175353\n",
      "[[ 5.63561916e-05 -1.45345926e-04 -6.81877136e-05 ... -5.32269478e-05\n",
      "  -1.88350677e-05  1.03414059e-05]\n",
      " [ 2.51531601e-05  4.08291817e-06 -6.37769699e-05 ... -9.78410244e-05\n",
      "   1.17883086e-04 -4.16040421e-05]\n",
      " [ 3.07857990e-05  3.15755606e-05 -6.27040863e-05 ... -1.45435333e-05\n",
      "  -3.48836184e-05 -7.14063644e-05]\n",
      " [ 8.72313976e-05  6.85453415e-07 -3.86536121e-05 ... -9.53450799e-05\n",
      "   8.67843628e-05 -5.93662262e-05]\n",
      " [ 1.25767663e-04  1.02996826e-04 -7.15255737e-07 ...  9.35196877e-05\n",
      "  -2.11298466e-05 -1.33275986e-04]]\n",
      "Flax relprop: 5\n",
      "Pt relprop: 5\n",
      "Relprop size: torch.Size([5, 768]) . Cam sum: 1.0000045 tensor(1., grad_fn=<SumBackward0>)\n",
      "Relprop diff: 1.5915419e-05\n",
      "torch.float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "all_head_size = configuration.num_attention_heads * int(configuration.hidden_size / configuration.num_attention_heads)\n",
    "q = tl.Linear(configuration.hidden_size, all_head_size).eval()\n",
    "k = fl.Dense(configuration.hidden_size, all_head_size)\n",
    "inputs = np.random.rand(*fl_out.shape)\n",
    "fl_in = jnp.array(inputs)\n",
    "pt_in =  torch.Tensor(inputs)\n",
    "lpt_out, lfl_out = verify_module_args(q, k, pt_in, fl_in, debug=True)\n",
    "print(lpt_out.dtype)\n",
    "print(lfl_out.dtype)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.query.weight torch.Size([768, 768])\n",
      "self.query.bias torch.Size([768])\n",
      "self.key.weight torch.Size([768, 768])\n",
      "self.key.bias torch.Size([768])\n",
      "self.value.weight torch.Size([768, 768])\n",
      "self.value.bias torch.Size([768])\n",
      "output.dense.weight torch.Size([768, 768])\n",
      "output.dense.bias torch.Size([768])\n",
      "output.LayerNorm.weight torch.Size([768])\n",
      "output.LayerNorm.bias torch.Size([768])\n",
      "Num Prams:  2363904\n",
      "+-------------------------------+------------+---------+-----------+--------+\n",
      "| Name                          | Shape      | Size    | Mean      | Std    |\n",
      "+-------------------------------+------------+---------+-----------+--------+\n",
      "| params/output/LayerNorm/bias  | (768,)     | 768     | 0.0       | 0.0    |\n",
      "| params/output/LayerNorm/scale | (768,)     | 768     | 1.0       | 0.0    |\n",
      "| params/output/dense/bias      | (768,)     | 768     | 0.000191  | 0.0208 |\n",
      "| params/output/dense/kernel    | (768, 768) | 589,824 | 4.15e-06  | 0.0208 |\n",
      "| params/self/key/bias          | (768,)     | 768     | -0.000446 | 0.021  |\n",
      "| params/self/key/kernel        | (768, 768) | 589,824 | 2.23e-06  | 0.0208 |\n",
      "| params/self/query/bias        | (768,)     | 768     | -0.000749 | 0.0201 |\n",
      "| params/self/query/kernel      | (768, 768) | 589,824 | 5.87e-06  | 0.0208 |\n",
      "| params/self/value/bias        | (768,)     | 768     | -5.31e-05 | 0.0207 |\n",
      "| params/self/value/kernel      | (768, 768) | 589,824 | 5.77e-06  | 0.0208 |\n",
      "+-------------------------------+------------+---------+-----------+--------+\n",
      "Total: 2,363,904\n",
      "Forward diff:  0.64730865\n",
      "Flax relprop: 1\n",
      "Pt relprop: 1\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop diff: 0.34968793\n",
      "Cam sum: tensor(0.9348, grad_fn=<SumBackward0>) 0.9999548\n"
     ]
    }
   ],
   "source": [
    "# <- BertEmbeddings\n",
    "pt_m = BertAttention(configuration).eval()\n",
    "fl_m = FlaxBertAttention(configuration)\n",
    "inputs = np.random.rand(*fl_out.shape)\n",
    "fl_in = {'hidden_states': jnp.array(inputs)}\n",
    "\n",
    "pt_in = {'hidden_states': torch.Tensor(inputs)}\n",
    "\n",
    "# print(np.sum(np.array(fl_in['hidden_states']) - pt_in['hidden_states'].numpy()))\n",
    "fl_in['layer_head_mask'] = None\n",
    "pt_in['head_mask'] = None\n",
    "\n",
    "fl_in['attention_mask'] = fl_inputs['attention_mask']\n",
    "pt_in['attention_mask'] = torch.Tensor(pt_inputs['attention_mask'].numpy())\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in, debug=True)\n",
    "att_in = fl_in"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense.weight torch.Size([3072, 768])\n",
      "dense.bias torch.Size([3072])\n",
      "Num Prams:  2362368\n",
      "+---------------------+-------------+-----------+----------+--------+\n",
      "| Name                | Shape       | Size      | Mean     | Std    |\n",
      "+---------------------+-------------+-----------+----------+--------+\n",
      "| params/dense/bias   | (3072,)     | 3,072     | -6.8e-05 | 0.021  |\n",
      "| params/dense/kernel | (768, 3072) | 2,359,296 | 1e-05    | 0.0208 |\n",
      "+---------------------+-------------+-----------+----------+--------+\n",
      "Total: 2,362,368\n",
      "Forward diff:  0.64422125\n",
      "Flax relprop: 1\n",
      "Pt relprop: 1\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop diff: 9.828516e-06\n",
      "Cam sum: tensor(1., grad_fn=<SumBackward0>) 0.99999726\n",
      "torch.Size([1, 5, 3072])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Verified\"\"\"\n",
    "# <- BertAttention\n",
    "pt_m = BertIntermediate(configuration).eval()\n",
    "fl_m = FlaxBertIntermediate(configuration)\n",
    "# inputs = np.random.rand(*fl_out.shape)\n",
    "att_out = np.random.rand(*fl_out.shape)\n",
    "\n",
    "fl_in = {'hidden_states': jnp.array(att_out)}\n",
    "pt_in = {'hidden_states': torch.Tensor(att_out)}\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in, debug=True)\n",
    "print(pt_out.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 3072)\n",
      "(1, 5, 768)\n",
      "dense.weight torch.Size([768, 3072])\n",
      "dense.bias torch.Size([768])\n",
      "LayerNorm.weight torch.Size([768])\n",
      "LayerNorm.bias torch.Size([768])\n",
      "Num Prams:  2361600\n",
      "+------------------------+-------------+-----------+-----------+--------+\n",
      "| Name                   | Shape       | Size      | Mean      | Std    |\n",
      "+------------------------+-------------+-----------+-----------+--------+\n",
      "| params/LayerNorm/bias  | (768,)      | 768       | 0.0       | 0.0    |\n",
      "| params/LayerNorm/scale | (768,)      | 768       | 1.0       | 0.0    |\n",
      "| params/dense/bias      | (768,)      | 768       | 0.000673  | 0.0106 |\n",
      "| params/dense/kernel    | (3072, 768) | 2,359,296 | -2.99e-06 | 0.0104 |\n",
      "+------------------------+-------------+-----------+-----------+--------+\n",
      "Total: 2,361,600\n",
      "Forward diff:  0.64069426\n",
      "Flax relprop: 2\n",
      "Pt relprop: 2\n",
      "Relprop size: torch.Size([1, 5, 3072])\n",
      "Relprop 0  diff: 0.20483156\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop 1  diff: 0.21466121\n",
      "Cam sum: tensor(1., grad_fn=<AddBackward0>) 0.99993986\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Verified??\"\"\"\n",
    "#TODO: make sure it's verified\n",
    "pt_m = BertOutput(configuration).eval()\n",
    "fl_m = FlaxBertOutput(configuration)\n",
    "# inputs = np.random.rand(*fl_out.shape)\n",
    "# att_out = np.random.rand(*att_out.shape)\n",
    "inputs = np.random.rand(1,5,3072)\n",
    "att_out = np.random.rand(1,5,768)\n",
    "print(inputs.shape)\n",
    "print(att_out.shape)\n",
    "\n",
    "fl_in = {'hidden_states': jnp.array(inputs)}\n",
    "pt_in = {'hidden_states': torch.Tensor(inputs)}\n",
    "\n",
    "# print(np.sum(np.array(fl_in['hidden_states']) - pt_in['hidden_states'].numpy()))\n",
    "# fl_in['layer_head_mask'] = None\n",
    "# pt_in['head_mask'] = None\n",
    "\n",
    "fl_in['attention_output'] = jnp.array(att_out)\n",
    "pt_in['input_tensor'] = torch.Tensor(att_out)\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in, debug=True)\n",
    "print(pt_out.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.weight torch.Size([768, 768])\n",
      "query.bias torch.Size([768])\n",
      "key.weight torch.Size([768, 768])\n",
      "key.bias torch.Size([768])\n",
      "value.weight torch.Size([768, 768])\n",
      "value.bias torch.Size([768])\n",
      "Num Prams:  1771776\n",
      "+---------------------+------------+---------+-----------+--------+\n",
      "| Name                | Shape      | Size    | Mean      | Std    |\n",
      "+---------------------+------------+---------+-----------+--------+\n",
      "| params/key/bias     | (768,)     | 768     | -0.00107  | 0.0211 |\n",
      "| params/key/kernel   | (768, 768) | 589,824 | -4.47e-05 | 0.0208 |\n",
      "| params/query/bias   | (768,)     | 768     | -0.000719 | 0.0209 |\n",
      "| params/query/kernel | (768, 768) | 589,824 | -2.14e-05 | 0.0208 |\n",
      "| params/value/bias   | (768,)     | 768     | -0.000857 | 0.0208 |\n",
      "| params/value/kernel | (768, 768) | 589,824 | 3.79e-05  | 0.0208 |\n",
      "+---------------------+------------+---------+-----------+--------+\n",
      "Total: 1,771,776\n",
      "Forward diff:  0.24423632\n",
      "Flax relprop: 1\n",
      "Pt relprop: 1\n",
      "Relprop diff: 2.471684\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "#TODO: not verified\n",
    "pt_m = BertSelfAttention(configuration).eval()\n",
    "fl_m = FlaxBertSelfAttention(configuration)\n",
    "inputs = np.random.rand(*fl_out.shape)\n",
    "fl_in = {'hidden_states': jnp.array(inputs)}\n",
    "pt_in = {'hidden_states': torch.Tensor(inputs)}\n",
    "\n",
    "# print(np.sum(np.array(fl_in['hidden_states']) - pt_in['hidden_states'].numpy()))\n",
    "fl_in['layer_head_mask'] = None\n",
    "pt_in['head_mask'] = None\n",
    "\n",
    "fl_in['attention_mask'] = fl_inputs['attention_mask']\n",
    "pt_in['attention_mask'] = torch.Tensor(pt_inputs['attention_mask'].numpy())\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in, debug=True)\n",
    "print(pt_out.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense.weight torch.Size([768, 768])\n",
      "dense.bias torch.Size([768])\n",
      "LayerNorm.weight torch.Size([768])\n",
      "LayerNorm.bias torch.Size([768])\n",
      "Num Prams:  592128\n",
      "+------------------------+------------+---------+-----------+--------+\n",
      "| Name                   | Shape      | Size    | Mean      | Std    |\n",
      "+------------------------+------------+---------+-----------+--------+\n",
      "| params/LayerNorm/bias  | (768,)     | 768     | 0.0       | 0.0    |\n",
      "| params/LayerNorm/scale | (768,)     | 768     | 1.0       | 0.0    |\n",
      "| params/dense/bias      | (768,)     | 768     | 0.000911  | 0.0207 |\n",
      "| params/dense/kernel    | (768, 768) | 589,824 | -4.27e-06 | 0.0208 |\n",
      "+------------------------+------------+---------+-----------+--------+\n",
      "Total: 592,128\n",
      "Forward diff:  0.6735077\n",
      "Flax relprop: 2\n",
      "Pt relprop: 2\n",
      "Relprop 0  diff: 0.07470034\n",
      "Relprop 1  diff: 0.11423703\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Verified?\"\"\"\n",
    "pt_m = BertSelfOutput(configuration).eval()\n",
    "fl_m = FlaxBertSelfOutput(configuration)\n",
    "inputs = np.random.rand(*fl_in['hidden_states'].shape)\n",
    "att_out = np.random.rand(*fl_out.shape)\n",
    "\n",
    "fl_in = {'hidden_states': jnp.array(inputs)}\n",
    "pt_in = {'hidden_states': torch.Tensor(inputs)}\n",
    "\n",
    "# print(np.sum(np.array(fl_in['hidden_states']) - pt_in['hidden_states'].numpy()))\n",
    "# fl_in['layer_head_mask'] = None\n",
    "# pt_in['head_mask'] = None\n",
    "\n",
    "fl_in['input_tensor'] = jnp.array(att_out)\n",
    "pt_in['input_tensor'] = torch.Tensor(att_out)\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in, debug=True)\n",
    "print(pt_out.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward diff:  0.7278961\n",
      "Flax relprop: 1\n",
      "Pt relprop: 1\n",
      "Relprop diff: 2.263708\n"
     ]
    }
   ],
   "source": [
    "pt_m = BertLayer(configuration).eval()\n",
    "fl_m = FlaxBertLayer(configuration)\n",
    "inputs = np.random.rand(*fl_out.shape)\n",
    "fl_in = {'hidden_states': jnp.array(inputs)}\n",
    "pt_in = {'hidden_states': torch.Tensor(inputs)}\n",
    "\n",
    "fl_in['layer_head_mask'] = None\n",
    "pt_in['head_mask'] = None\n",
    "\n",
    "fl_in['attention_mask'] = fl_inputs['attention_mask']\n",
    "pt_in['attention_mask'] = torch.Tensor(pt_inputs['attention_mask'].numpy())\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward diff:  1.5823481\n",
      "Flax relprop: 1\n",
      "Pt relprop: 1\n",
      "Relprop diff: 2.1209507\n"
     ]
    }
   ],
   "source": [
    "pt_m = BertEncoder(configuration).eval()\n",
    "fl_m = FlaxBertEncoder(configuration)\n",
    "\n",
    "# requires: hidden_states, attention_mask, head_mask (default None)\n",
    "inputs = np.random.rand(*fl_out.shape)\n",
    "fl_in = {'hidden_states': jnp.array(inputs)}\n",
    "pt_in = {'hidden_states': torch.Tensor(inputs)}\n",
    "\n",
    "fl_in['head_mask'] = None\n",
    "pt_in['head_mask'] = None\n",
    "\n",
    "fl_in['attention_mask'] = fl_inputs['attention_mask']\n",
    "pt_in['attention_mask'] = torch.Tensor(pt_inputs['attention_mask'].numpy())\n",
    "\n",
    "# print()\n",
    "\n",
    "\n",
    "# print(fl_in, pt_in)\n",
    "# fl_in.pop('attention_mask', None)\n",
    "# pt_in.pop('attention_mask', None)\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# fl_in = (input_ids, token_ids, position_ids, attention_mask)\n",
    "# pt_in = (pt_input_ids, pt_token_ids, pt_position_ids, None)\n",
    "# fl_in = fl_inputs\n",
    "# pt_in = pt_inputs\n",
    "# pt_out, fl_out = verify_module(pt_layer, fl_layer, pt_in, fl_in)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenyi/anaconda3/envs/dlsys/lib/python3.9/site-packages/transformers/modeling_utils.py:735: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward diff:  0.19136032\n",
      "Flax relprop: 1\n",
      "Pt relprop: 1\n",
      "Relprop diff: 2.0647373\n"
     ]
    }
   ],
   "source": [
    "pt_m = BertModel(configuration).eval()\n",
    "fl_m = FlaxBertModule(configuration)\n",
    "fl_in = fl_inputs\n",
    "pt_in = pt_inputs\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(parameter_overview.get_parameter_overview())\n",
    "print(pt_layer, count_parameters(pt_layer))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for k,v in pt_layer.named_parameters():\n",
    "    print(k, v.shape)\n",
    "\n",
    "vf = {'params':\n",
    "          {'word_embeddings':\n",
    "               {'embedding': None},\n",
    "           'position_embeddings':\n",
    "               {'embedding': None},\n",
    "           'token_type_embeddings':\n",
    "               {'embedding': None},\n",
    "           'LayerNorm':\n",
    "               {'bias': None,\n",
    "                'scale': None},\n",
    "        }\n",
    "      }\n",
    "for k,v in pt_layer.named_parameters():\n",
    "    if k == 'word_embeddings.weight':\n",
    "        vf['params']['word_embeddings']['embedding'] = v.detach().numpy()\n",
    "    if k == 'position_embeddings.weight':\n",
    "        vf['params']['position_embeddings']['embedding'] = v.detach().numpy()\n",
    "    if k == 'token_type_embeddings.weight':\n",
    "        vf['params']['token_type_embeddings']['embedding'] = v.detach().numpy()\n",
    "    if k == 'LayerNorm.weight':\n",
    "        vf['params']['LayerNorm']['scale'] = v.detach().numpy()\n",
    "    if k == 'LayerNorm.bias':\n",
    "        vf['params']['LayerNorm']['bias'] = v.detach().numpy()\n",
    "\n",
    "print(vf)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# vf = f_layer.init(jax.random.PRNGKey(0), input_ids, token_ids,  position_ids, attention_mask)\n",
    "f_out = f_layer.apply(vf, input_ids, token_ids, position_ids, attention_mask)\n",
    "pt_out = pt_layer(input_ids=pt_input_ids, token_type_ids=pt_token_ids, position_ids=pt_position_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30522, 768)\n",
      "(1, 5, 768)\n",
      "torch.Size([1, 5, 768])\n",
      "3.368198e-05\n"
     ]
    }
   ],
   "source": [
    "print(vf['params']['word_embeddings']['embedding'].shape)\n",
    "print(f_out.shape)\n",
    "print(pt_out.shape)\n",
    "print(np.abs(pt_out.detach().numpy() - f_out).sum())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relprop 2\n",
      "2.7899752\n",
      "2.787298\n",
      "2.7194073\n",
      "1.0000002\n",
      "1.0\n",
      "(1, 5, 768)\n"
     ]
    }
   ],
   "source": [
    "cam = np.random.rand(*f_out.shape)\n",
    "f_cam = jnp.array( cam / cam.sum())\n",
    "pt_cam = torch.tensor(cam / cam.sum())\n",
    "\n",
    "f_cam1, f_cam2 = f_layer.apply(vf, f_cam, input_ids, token_ids, position_ids, attention_mask, method=f_layer.relprop)\n",
    "kwargs = {'alpha': 1}\n",
    "pt_cam1, pt_cam2 = pt_layer.relprop(pt_cam, **kwargs)\n",
    "\n",
    "\n",
    "print(np.abs(np.array(f_cam1) - pt_cam1.detach().numpy()).sum())\n",
    "print(np.abs(np.array(f_cam2) - pt_cam2.detach().numpy()).sum())\n",
    "print(np.abs(np.array(f_cam1) - pt_cam2.detach().numpy()).sum())\n",
    "print(np.sum(f_cam1) + np.sum(f_cam2))\n",
    "print(np.sum(pt_cam1.detach().numpy()) + np.sum(pt_cam2.detach().numpy()))\n",
    "print(f_cam2.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+--------------+------------+-----------+-------+\n",
      "| Name                                   | Shape        | Size       | Mean      | Std   |\n",
      "+----------------------------------------+--------------+------------+-----------+-------+\n",
      "| params/LayerNorm/bias                  | (768,)       | 768        | 0.0       | 0.0   |\n",
      "| params/LayerNorm/scale                 | (768,)       | 768        | 1.0       | 0.0   |\n",
      "| params/position_embeddings/embedding   | (512, 768)   | 393,216    | -0.00286  | 0.999 |\n",
      "| params/token_type_embeddings/embedding | (2, 768)     | 1,536      | -0.00341  | 1.01  |\n",
      "| params/word_embeddings/embedding       | (30522, 768) | 23,440,896 | -7.33e-05 | 1.0   |\n",
      "+----------------------------------------+--------------+------------+-----------+-------+\n",
      "Total: 23,837,184\n",
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (add1): Add()\n",
      "  (add2): Add()\n",
      ") 23837184\n"
     ]
    }
   ],
   "source": [
    "print(parameter_overview.get_parameter_overview(vf))\n",
    "print(pt_layer, count_parameters(pt_layer))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "pt_layer = BertEncoder(configuration).eval()\n",
    "f_layer = FlaxBertEncoder(configuration)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.0.attention.self.query.bias torch.Size([768])\n",
      "layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.0.attention.self.key.bias torch.Size([768])\n",
      "layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.0.attention.self.value.bias torch.Size([768])\n",
      "layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.0.attention.output.dense.bias torch.Size([768])\n",
      "layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "layer.0.output.dense.bias torch.Size([768])\n",
      "layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.1.attention.self.query.bias torch.Size([768])\n",
      "layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.1.attention.self.key.bias torch.Size([768])\n",
      "layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.1.attention.self.value.bias torch.Size([768])\n",
      "layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.1.attention.output.dense.bias torch.Size([768])\n",
      "layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "layer.1.output.dense.bias torch.Size([768])\n",
      "layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.2.attention.self.query.bias torch.Size([768])\n",
      "layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.2.attention.self.key.bias torch.Size([768])\n",
      "layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.2.attention.self.value.bias torch.Size([768])\n",
      "layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.2.attention.output.dense.bias torch.Size([768])\n",
      "layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "layer.2.output.dense.bias torch.Size([768])\n",
      "layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.3.attention.self.query.bias torch.Size([768])\n",
      "layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.3.attention.self.key.bias torch.Size([768])\n",
      "layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.3.attention.self.value.bias torch.Size([768])\n",
      "layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.3.attention.output.dense.bias torch.Size([768])\n",
      "layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "layer.3.output.dense.bias torch.Size([768])\n",
      "layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.4.attention.self.query.bias torch.Size([768])\n",
      "layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.4.attention.self.key.bias torch.Size([768])\n",
      "layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.4.attention.self.value.bias torch.Size([768])\n",
      "layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.4.attention.output.dense.bias torch.Size([768])\n",
      "layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "layer.4.output.dense.bias torch.Size([768])\n",
      "layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.5.attention.self.query.bias torch.Size([768])\n",
      "layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.5.attention.self.key.bias torch.Size([768])\n",
      "layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.5.attention.self.value.bias torch.Size([768])\n",
      "layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.5.attention.output.dense.bias torch.Size([768])\n",
      "layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "layer.5.output.dense.bias torch.Size([768])\n",
      "layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.6.attention.self.query.bias torch.Size([768])\n",
      "layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.6.attention.self.key.bias torch.Size([768])\n",
      "layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.6.attention.self.value.bias torch.Size([768])\n",
      "layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.6.attention.output.dense.bias torch.Size([768])\n",
      "layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "layer.6.output.dense.bias torch.Size([768])\n",
      "layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.7.attention.self.query.bias torch.Size([768])\n",
      "layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.7.attention.self.key.bias torch.Size([768])\n",
      "layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.7.attention.self.value.bias torch.Size([768])\n",
      "layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.7.attention.output.dense.bias torch.Size([768])\n",
      "layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "layer.7.output.dense.bias torch.Size([768])\n",
      "layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.8.attention.self.query.bias torch.Size([768])\n",
      "layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.8.attention.self.key.bias torch.Size([768])\n",
      "layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.8.attention.self.value.bias torch.Size([768])\n",
      "layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.8.attention.output.dense.bias torch.Size([768])\n",
      "layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "layer.8.output.dense.bias torch.Size([768])\n",
      "layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.9.attention.self.query.bias torch.Size([768])\n",
      "layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.9.attention.self.key.bias torch.Size([768])\n",
      "layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.9.attention.self.value.bias torch.Size([768])\n",
      "layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.9.attention.output.dense.bias torch.Size([768])\n",
      "layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "layer.9.output.dense.bias torch.Size([768])\n",
      "layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.10.attention.self.query.bias torch.Size([768])\n",
      "layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.10.attention.self.key.bias torch.Size([768])\n",
      "layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.10.attention.self.value.bias torch.Size([768])\n",
      "layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.10.attention.output.dense.bias torch.Size([768])\n",
      "layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "layer.10.output.dense.bias torch.Size([768])\n",
      "layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.11.attention.self.query.bias torch.Size([768])\n",
      "layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.11.attention.self.key.bias torch.Size([768])\n",
      "layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.11.attention.self.value.bias torch.Size([768])\n",
      "layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.11.attention.output.dense.bias torch.Size([768])\n",
      "layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "layer.11.output.dense.bias torch.Size([768])\n",
      "layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "layer.11.output.LayerNorm.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for k,v in pt_layer.named_parameters():\n",
    "    print(k, v.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
