{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules needed to be verified\n",
    "\n",
    "\n",
    "## Basic Building Blocks (bb)\n",
    "all the components in the layers file\n",
    "\n",
    "- FlaxBertForSequenceClassification\n",
    "    - FlaxBertForSequenceClassificationModule\n",
    "        - FlaxBertModule\n",
    "            - FlaxBertEmbeddings\n",
    "                - Add\n",
    "                - LayerNorm\n",
    "                - Dropout\n",
    "            - FlaxBertEncoder\n",
    "                - FlaxBertLayerCollection\n",
    "                    - FlaxBertLayer\n",
    "                        - FlaxBertAttention\n",
    "                            - FlaxBertSelfAttention(bb only)\n",
    "                            - ~~FlaxBertSelfOutput(bb only)~~ ?\n",
    "                        - ~~FlaxBertIntermediate(bb only)~~\n",
    "                        - FlaxBertOutput(bb only)\n",
    "                    - FlaxBertCheckpointLayer(cond.)\n",
    "            - FlaxBertPooler(bb only)\n",
    "        - Dropout\n",
    "        - Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Building Block verification.\n",
    "- FlaxBertSelfAttention vs.     BertSelfAttention\n",
    "- FlaxBertSelfOutput    vs.     BertSelfOutput\n",
    "- FlaxBertIntermediate  vs.     BertIntermediate\n",
    "- FlaxBertOutput        vs.     BertOutput\n",
    "- FlaxBertPooler        vs.     BertPooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/.conda/envs/dl_sys/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-12-04 15:44:33.955805: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-04 15:44:33.979116: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-04 15:44:34.430193: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-04 15:44:34.430259: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-04 15:44:34.430267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from clu import parameter_overview\n",
    "import flax.linen as fnn\n",
    "\n",
    "sys.path.insert(0, '../bert')\n",
    "sys.path.insert(0, '../bert_torch')\n",
    "from BERT import *\n",
    "from modeling_flax_bert import *\n",
    "import modeling_flax_bert as layers\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "import layers as tl\n",
    "import bert_explainability_layers as fl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "configuration = BertConfig()\n",
    "print(configuration.add_cross_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': DeviceArray([[ 101, 7592, 2088,  999,  102]], dtype=int32), 'token_type_ids': DeviceArray([[0, 0, 0, 0, 0]], dtype=int32), 'attention_mask': DeviceArray([[1, 1, 1, 1, 1]], dtype=int32), 'position_ids': DeviceArray([0], dtype=int32)} {'input_ids': tensor([[ 101, 7592, 2088,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'position_ids': tensor([0])}\n"
     ]
    }
   ],
   "source": [
    "text_batch = [\"Hello world!\"]\n",
    "fl_inputs = tokenizer(text_batch)\n",
    "# print(fl_inputs.__class__)\n",
    "# print(dict(inputs).__class__)\n",
    "\n",
    "for k in fl_inputs:\n",
    "    fl_inputs[k] = jnp.array(fl_inputs[k])\n",
    "fl_inputs['position_ids'] = jnp.arange(fl_inputs['input_ids'].shape[0])\n",
    "\n",
    "input_ids = jnp.array(fl_inputs[\"input_ids\"])\n",
    "token_ids = jnp.array(fl_inputs[\"token_type_ids\"])\n",
    "attention_mask = jnp.array(fl_inputs[\"attention_mask\"])\n",
    "position_ids = jnp.arange(input_ids.shape[0])\n",
    "\n",
    "pt_inputs = tokenizer(text_batch, return_tensors='pt')\n",
    "pt_inputs['position_ids'] = torch.tensor(np.arange(pt_inputs['input_ids'].shape[0]))\n",
    "pt_input_ids = pt_inputs[\"input_ids\"]\n",
    "pt_token_ids = pt_inputs[\"token_type_ids\"]\n",
    "pt_attention_mask = pt_inputs[\"attention_mask\"]\n",
    "pt_position_ids = torch.tensor(np.arange(pt_input_ids.shape[0]))\n",
    "\n",
    "print(fl_inputs, pt_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt_layer = BertEmbeddings(configuration).eval()\n",
    "# fl_layer = FlaxBertEmbeddings(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_set(dic, keys, value, create_missing=True):\n",
    "    d = dic\n",
    "    for key in keys[:-1]:\n",
    "        if key in d:\n",
    "            d = d[key]\n",
    "        elif create_missing:\n",
    "            d = d.setdefault(key, {})\n",
    "        else:\n",
    "            return dic\n",
    "    if keys[-1] in d or create_missing:\n",
    "        d[keys[-1]] = value\n",
    "    return dic\n",
    "\n",
    "\n",
    "def pt2fl(pt: nn.Module, debug=False):\n",
    "    vf = {}\n",
    "    for k, v in pt.named_parameters():\n",
    "        keys = k.split('.')\n",
    "        if len(keys) >= 2:\n",
    "            if debug:\n",
    "                print(k, v.size())\n",
    "            if keys[-2].endswith('embeddings'):\n",
    "                keys[-1] = 'embedding'\n",
    "\n",
    "            if keys[-1] == 'weight':\n",
    "                if keys[-2] == 'LayerNorm':\n",
    "                    keys[-1] = 'scale'\n",
    "                else:\n",
    "                    keys[-1] = 'kernel'\n",
    "\n",
    "            if keys[-2] in ['dense', 'key', 'value', 'query'] and keys[-1] == 'kernel':\n",
    "                nested_set(vf, keys, value=jnp.transpose(v.detach().numpy(), (1, 0)))\n",
    "            else:\n",
    "                nested_set(vf, keys, value=v.detach().numpy())\n",
    "        else:\n",
    "            if keys[-1] == 'weight':\n",
    "                keys[-1] = 'kernel'\n",
    "                nested_set(vf, keys, value=jnp.transpose(v.detach().numpy(), (1, 0)))\n",
    "            else:\n",
    "                nested_set(vf, keys, value=v.detach().numpy())\n",
    "    params = {'params': vf}\n",
    "    if debug:\n",
    "        print('Num Prams: ', count_parameters(pt))\n",
    "        print(parameter_overview.get_parameter_overview(params))\n",
    "    return params\n",
    "\n",
    "\n",
    "def verify_module(pt:nn.Module, fl:fnn.Module, pt_kwargs=None, fl_kwargs=None, debug=False):\n",
    "    params = pt2fl(pt, debug=debug)\n",
    "\n",
    "    pt_out = pt(**pt_kwargs)\n",
    "    fl_out = fl.apply(params, **fl_kwargs)\n",
    "    # print(pt_out)\n",
    "    if isinstance(pt_out, tuple):\n",
    "        pt_out = pt_out[0]\n",
    "    if isinstance(fl_out, tuple):\n",
    "        fl_out = fl_out[0]\n",
    "    if isinstance(pt_out, BaseModelOutput):\n",
    "        pt_out = pt_out[0]\n",
    "\n",
    "    if isinstance(fl_out, FlaxBaseModelOutputWithPastAndCrossAttentions):\n",
    "        fl_out = fl_out[0]\n",
    "    if isinstance(pt_out, BaseModelOutputWithPooling):\n",
    "        pt_out = pt_out[1]\n",
    "    if isinstance(fl_out, FlaxBaseModelOutputWithPoolingAndCrossAttentions):\n",
    "        fl_out = fl_out[1]\n",
    "    print(\"Forward diff: \",np.abs(pt_out.detach().numpy() - fl_out).sum())\n",
    "\n",
    "    # print(parameter_overview.get_parameter_overview(params))\n",
    "    # print(pt, count_parameters(pt))\n",
    "\n",
    "    cam = np.random.rand(*fl_out.shape)\n",
    "    fl_cam = jnp.array(cam / cam.sum())\n",
    "    pt_cam = torch.tensor(cam / cam.sum())\n",
    "\n",
    "    fl_cams = fl.apply(params, fl_cam, **fl_kwargs, method=fl.relprop)\n",
    "    kwargs = {'alpha': 1}\n",
    "    pt_cams = pt.relprop(pt_cam, **kwargs)\n",
    "\n",
    "    print('Flax relprop:', fl_cams)\n",
    "    print('Pt relprop:', pt_cams)\n",
    "    pt_sum = None\n",
    "    fl_sum = None\n",
    "    if isinstance(fl_cams, list) or isinstance(fl_cams, tuple):\n",
    "\n",
    "        for i, c in enumerate(fl_cams):\n",
    "            print(\"Relprop size:\", pt_cams[i].size())\n",
    "            print(\"Relprop\", i, \" diff:\", np.abs(np.array(c) - pt_cams[i].detach().numpy()).sum())\n",
    "            if pt_sum:\n",
    "                pt_sum+=pt_cams[i].sum()\n",
    "            else:\n",
    "                pt_sum = pt_cams[i].sum()\n",
    "\n",
    "            if fl_sum:\n",
    "                fl_sum+=c.sum()\n",
    "            else:\n",
    "                fl_sum = c.sum()\n",
    "    else:\n",
    "        pt_sum = pt_cams.sum()\n",
    "        fl_sum = fl_cams.sum()\n",
    "        print(\"Relprop size:\", pt_cams.size())\n",
    "        print(\"Relprop diff:\", np.abs(np.array(fl_cams) - pt_cams.detach().numpy()).sum())\n",
    "    print(\"Cam sum:\", pt_sum, fl_sum)\n",
    "    return pt_cams, fl_cams\n",
    "\n",
    "def verify_module_args(pt:nn.Module, fl:fnn.Module, pt_kwargs=None, fl_kwargs=None, debug=False):\n",
    "    params = pt2fl(pt, debug=debug)\n",
    "\n",
    "    pt_out = pt(*pt_kwargs)\n",
    "    fl_out = fl.apply(params, *fl_kwargs)\n",
    "    # print(pt_out)\n",
    "    if isinstance(pt_out, tuple):\n",
    "        pt_out = pt_out[0]\n",
    "    if isinstance(fl_out, tuple):\n",
    "        fl_out = fl_out[0]\n",
    "    if isinstance(pt_out, BaseModelOutput):\n",
    "        pt_out = pt_out[0]\n",
    "\n",
    "    if isinstance(fl_out, FlaxBaseModelOutputWithPastAndCrossAttentions):\n",
    "        fl_out = fl_out[0]\n",
    "    if isinstance(pt_out, BaseModelOutputWithPooling):\n",
    "        pt_out = pt_out[1]\n",
    "    if isinstance(fl_out, FlaxBaseModelOutputWithPoolingAndCrossAttentions):\n",
    "        fl_out = fl_out[1]\n",
    "    print(\"Forward diff: \",np.abs(pt_out.detach().numpy() - fl_out).sum())\n",
    "    if debug:\n",
    "        # print(fl_out)\n",
    "        # print(pt_out.detach().numpy())\n",
    "        print(pt_out.detach().numpy() - fl_out)\n",
    "    # print(parameter_overview.get_parameter_overview(params))\n",
    "    # print(pt, count_parameters(pt))\n",
    "\n",
    "    cam = np.random.rand(*fl_out.shape)\n",
    "    fl_cam = jnp.array(cam / cam.sum())\n",
    "    pt_cam = torch.tensor(cam / cam.sum())\n",
    "\n",
    "    fl_cams = fl.apply(params, fl_cam, *fl_kwargs, method=fl.relprop)\n",
    "    kwargs = {'alpha': 1}\n",
    "    pt_cams = pt.relprop(pt_cam, **kwargs)\n",
    "\n",
    "    print('Flax relprop:', len(fl_cams))\n",
    "    print('Pt relprop:', len(pt_cams))\n",
    "\n",
    "    if isinstance(fl_cams, list) or isinstance(fl_cams, tuple):\n",
    "        for i, c in enumerate(fl_cams):\n",
    "            print(\"Relprop size:\", pt_cams[i].size())\n",
    "            print(\"Relprop\", i, \" diff:\", np.abs(np.array(c) - pt_cams[i].detach().numpy()).sum())\n",
    "    else:\n",
    "        print(\"Relprop size:\", pt_cams.size(), \". Cam sum:\", fl_cams.sum(), pt_cams.sum() )\n",
    "        print(\"Relprop diff:\", np.abs(np.array(fl_cams) - pt_cams.detach().numpy()).sum())\n",
    "    return pt_out, fl_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': DeviceArray([[ 101, 7592, 2088,  999,  102]], dtype=int32), 'token_type_ids': DeviceArray([[0, 0, 0, 0, 0]], dtype=int32), 'attention_mask': DeviceArray([[1, 1, 1, 1, 1]], dtype=int32), 'position_ids': DeviceArray([0], dtype=int32)} {'input_ids': tensor([[ 101, 7592, 2088,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'position_ids': tensor([0])}\n",
      "word_embeddings.weight torch.Size([30522, 768])\n",
      "position_embeddings.weight torch.Size([512, 768])\n",
      "token_type_embeddings.weight torch.Size([2, 768])\n",
      "LayerNorm.weight torch.Size([768])\n",
      "LayerNorm.bias torch.Size([768])\n",
      "Num Prams:  23837184\n",
      "+----------------------------------------+--------------+------------+-----------+-------+\n",
      "| Name                                   | Shape        | Size       | Mean      | Std   |\n",
      "+----------------------------------------+--------------+------------+-----------+-------+\n",
      "| params/LayerNorm/bias                  | (768,)       | 768        | 0.0       | 0.0   |\n",
      "| params/LayerNorm/scale                 | (768,)       | 768        | 1.0       | 0.0   |\n",
      "| params/position_embeddings/embedding   | (512, 768)   | 393,216    | -0.000232 | 0.999 |\n",
      "| params/token_type_embeddings/embedding | (2, 768)     | 1,536      | -0.013    | 1.0   |\n",
      "| params/word_embeddings/embedding       | (30522, 768) | 23,440,896 | 0.000366  | 1.0   |\n",
      "+----------------------------------------+--------------+------------+-----------+-------+\n",
      "Total: 23,837,184\n",
      "Forward diff:  0.00011428408\n",
      "Flax relprop: [DeviceArray([[[ 2.57516513e-05,  1.33581098e-05,  5.46310766e-05, ...,\n",
      "                3.47677633e-05,  3.02946835e-04,  1.46047663e-04],\n",
      "              [ 2.19156209e-04,  2.49083387e-05,  1.31712499e-04, ...,\n",
      "                9.97427414e-05,  2.83837959e-04,  8.10628844e-05],\n",
      "              [ 2.15930893e-04, -6.12160366e-05,  1.71995183e-04, ...,\n",
      "                9.27223937e-06,  1.55882663e-04,  1.10839246e-04],\n",
      "              [ 2.73513928e-04, -4.32534762e-05,  1.18632764e-04, ...,\n",
      "                1.46037215e-04,  1.87935620e-05,  3.94299655e-04],\n",
      "              [ 1.79900409e-04,  2.22571489e-05,  3.14417161e-06, ...,\n",
      "                7.75418594e-05,  9.27603687e-05,  1.47360144e-03]]],            dtype=float32), DeviceArray([[[-1.1784655e-05, -3.5397385e-05,  4.1697531e-06, ...,\n",
      "                2.5553534e-05,  7.9688412e-05,  5.9234321e-06],\n",
      "              [ 1.5355463e-05, -2.3835694e-04,  1.6130662e-05, ...,\n",
      "                5.5974164e-05,  9.1522335e-05, -1.0834695e-04],\n",
      "              [ 4.8167612e-06, -2.8268760e-04,  5.7514924e-05, ...,\n",
      "               -1.0731324e-05, -1.0583655e-05, -3.4011435e-05],\n",
      "              [ 2.6222233e-05, -2.5016724e-04, -9.7193697e-05, ...,\n",
      "                3.1117408e-05, -2.5341018e-05,  1.4314655e-04],\n",
      "              [-6.2963431e-05, -1.1183268e-04, -1.2583863e-06, ...,\n",
      "               -2.7484599e-05,  2.6749127e-05,  1.2802739e-03]]],            dtype=float32)]\n",
      "Pt relprop: [tensor([[[ 2.5752e-05,  1.3358e-05,  5.4631e-05,  ...,  3.4768e-05,\n",
      "           3.0295e-04,  1.4605e-04],\n",
      "         [ 2.1916e-04,  2.4908e-05,  1.3171e-04,  ...,  9.9743e-05,\n",
      "           2.8384e-04,  8.1063e-05],\n",
      "         [ 2.1593e-04, -6.1216e-05,  1.7200e-04,  ...,  9.2722e-06,\n",
      "           1.5588e-04,  1.1084e-04],\n",
      "         [ 2.7351e-04, -4.3253e-05,  1.1863e-04,  ...,  1.4604e-04,\n",
      "           1.8794e-05,  3.9430e-04],\n",
      "         [ 1.7990e-04,  2.2257e-05,  3.1442e-06,  ...,  7.7542e-05,\n",
      "           9.2760e-05,  1.4736e-03]]], grad_fn=<MulBackward0>), tensor([[[-1.1785e-05, -3.5397e-05,  4.1698e-06,  ...,  2.5554e-05,\n",
      "           7.9688e-05,  5.9234e-06],\n",
      "         [ 1.5355e-05, -2.3836e-04,  1.6131e-05,  ...,  5.5974e-05,\n",
      "           9.1522e-05, -1.0835e-04],\n",
      "         [ 4.8168e-06, -2.8269e-04,  5.7515e-05,  ..., -1.0731e-05,\n",
      "          -1.0584e-05, -3.4011e-05],\n",
      "         [ 2.6222e-05, -2.5017e-04, -9.7194e-05,  ...,  3.1117e-05,\n",
      "          -2.5341e-05,  1.4315e-04],\n",
      "         [-6.2963e-05, -1.1183e-04, -1.2584e-06,  ..., -2.7485e-05,\n",
      "           2.6749e-05,  1.2803e-03]]], grad_fn=<MulBackward0>)]\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop 0  diff: 2.0263691e-08\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop 1  diff: 1.739723e-08\n",
      "Cam sum: tensor(1.0000, grad_fn=<AddBackward0>) 0.9999999\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Verified\"\"\"\n",
    "pt_m = BertEmbeddings(configuration).eval()\n",
    "fl_m = FlaxBertEmbeddings(configuration)\n",
    "fl_in = fl_inputs.copy()\n",
    "pt_in = pt_inputs.copy()\n",
    "print(fl_in, pt_in)\n",
    "# fl_in.pop('attention_mask', None)\n",
    "pt_in.pop('attention_mask', None)\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in, debug=True)\n",
    "\n",
    "print(pt_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Prams:  590592\n",
      "+---------------+------------+---------+-----------+--------+\n",
      "| Name          | Shape      | Size    | Mean      | Std    |\n",
      "+---------------+------------+---------+-----------+--------+\n",
      "| params/bias   | (768,)     | 768     | -0.000511 | 0.0202 |\n",
      "| params/kernel | (768, 768) | 589,824 | 2.52e-05  | 0.0208 |\n",
      "+---------------+------------+---------+-----------+--------+\n",
      "Total: 590,592\n",
      "Forward diff:  0.00029218418\n",
      "[[-2.9802322e-08  1.7881393e-07  1.4901161e-08 ...  1.7881393e-07\n",
      "   2.9802322e-08  5.9604645e-08]\n",
      " [ 1.4901161e-08 -7.4505806e-09  1.1920929e-07 ... -2.9802322e-08\n",
      "   7.4505806e-08  5.9604645e-08]\n",
      " [-8.1956387e-08 -8.9406967e-08  1.1175871e-08 ...  2.3841858e-07\n",
      "  -8.1956387e-08  5.9604645e-08]\n",
      " [-2.2351742e-08  2.9802322e-08  4.4703484e-08 ... -5.9604645e-08\n",
      "  -2.9802322e-08  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00 -3.7252903e-09 ...  1.1920929e-07\n",
      "   1.7881393e-07  0.0000000e+00]]\n",
      "Flax relprop: 5\n",
      "Pt relprop: 5\n",
      "Relprop size: torch.Size([5, 768]) . Cam sum: 1.0 tensor(1., grad_fn=<SumBackward0>)\n",
      "Relprop diff: 8.429682e-08\n",
      "torch.float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "all_head_size = configuration.num_attention_heads * int(configuration.hidden_size / configuration.num_attention_heads)\n",
    "q = tl.Linear(configuration.hidden_size, all_head_size).eval()\n",
    "k = fl.Dense(configuration.hidden_size, all_head_size)\n",
    "inputs = np.random.rand(*fl_out.shape)\n",
    "fl_in = jnp.array(inputs)\n",
    "pt_in =  torch.Tensor(inputs)\n",
    "lpt_out, lfl_out = verify_module_args(q, k, pt_in, fl_in, debug=True)\n",
    "print(lpt_out.dtype)\n",
    "print(lfl_out.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.query.weight torch.Size([768, 768])\n",
      "self.query.bias torch.Size([768])\n",
      "self.key.weight torch.Size([768, 768])\n",
      "self.key.bias torch.Size([768])\n",
      "self.value.weight torch.Size([768, 768])\n",
      "self.value.bias torch.Size([768])\n",
      "output.dense.weight torch.Size([768, 768])\n",
      "output.dense.bias torch.Size([768])\n",
      "output.LayerNorm.weight torch.Size([768])\n",
      "output.LayerNorm.bias torch.Size([768])\n",
      "Num Prams:  2363904\n",
      "+-------------------------------+------------+---------+-----------+--------+\n",
      "| Name                          | Shape      | Size    | Mean      | Std    |\n",
      "+-------------------------------+------------+---------+-----------+--------+\n",
      "| params/output/LayerNorm/bias  | (768,)     | 768     | 0.0       | 0.0    |\n",
      "| params/output/LayerNorm/scale | (768,)     | 768     | 1.0       | 0.0    |\n",
      "| params/output/dense/bias      | (768,)     | 768     | -0.000703 | 0.0206 |\n",
      "| params/output/dense/kernel    | (768, 768) | 589,824 | 7.35e-05  | 0.0208 |\n",
      "| params/self/key/bias          | (768,)     | 768     | -0.000179 | 0.0205 |\n",
      "| params/self/key/kernel        | (768, 768) | 589,824 | -1.93e-05 | 0.0208 |\n",
      "| params/self/query/bias        | (768,)     | 768     | 0.00109   | 0.0208 |\n",
      "| params/self/query/kernel      | (768, 768) | 589,824 | 4.78e-07  | 0.0209 |\n",
      "| params/self/value/bias        | (768,)     | 768     | 0.000995  | 0.0205 |\n",
      "| params/self/value/kernel      | (768, 768) | 589,824 | 3.2e-05   | 0.0208 |\n",
      "+-------------------------------+------------+---------+-----------+--------+\n",
      "Total: 2,363,904\n",
      "Forward diff:  0.0009914429\n",
      "Flax relprop: [[[ 1.4342747e-04  6.2879152e-04  5.0754570e-05 ...  2.3594502e-04\n",
      "    7.4937234e-05  3.7863787e-04]\n",
      "  [ 3.7809252e-05  5.3967175e-04  1.1195316e-04 ...  1.5329257e-04\n",
      "    3.7823967e-04  3.1523983e-04]\n",
      "  [ 3.2891450e-05 -2.1711994e-04  4.5841391e-04 ...  3.9415236e-04\n",
      "    4.9209740e-04  6.4203661e-05]\n",
      "  [ 1.9066456e-04  1.2376490e-04  5.0731271e-04 ...  3.0394192e-04\n",
      "    3.6682759e-05  3.2909101e-04]\n",
      "  [ 1.9889872e-04  5.8298488e-04  8.0024445e-05 ...  4.5051007e-04\n",
      "    3.1030457e-04  2.3081985e-04]]]\n",
      "Pt relprop: tensor([[[ 1.4301e-04,  6.2660e-04,  5.0695e-05,  ...,  2.3538e-04,\n",
      "           7.4992e-05,  3.7772e-04],\n",
      "         [ 3.7989e-05,  5.3794e-04,  1.1161e-04,  ...,  1.5309e-04,\n",
      "           3.7704e-04,  3.1452e-04],\n",
      "         [ 3.2833e-05, -2.1630e-04,  4.5711e-04,  ...,  3.9299e-04,\n",
      "           4.9069e-04,  6.4299e-05],\n",
      "         [ 1.9046e-04,  1.2337e-04,  5.0567e-04,  ...,  3.0335e-04,\n",
      "           3.6956e-05,  3.2836e-04],\n",
      "         [ 1.9833e-04,  5.8109e-04,  8.0015e-05,  ...,  4.4947e-04,\n",
      "           3.0935e-04,  2.3036e-04]]], grad_fn=<MulBackward0>)\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop diff: 0.009917375\n",
      "Cam sum: tensor(0.9922, grad_fn=<SumBackward0>) 0.9930946\n"
     ]
    }
   ],
   "source": [
    "# <- BertEmbeddings\n",
    "pt_m = BertAttention(configuration).eval()\n",
    "fl_m = FlaxBertAttention(configuration)\n",
    "inputs = np.random.rand(*fl_out.shape)\n",
    "fl_in = {'hidden_states': jnp.array(inputs)}\n",
    "\n",
    "pt_in = {'hidden_states': torch.Tensor(inputs)}\n",
    "\n",
    "# print(np.sum(np.array(fl_in['hidden_states']) - pt_in['hidden_states'].numpy()))\n",
    "fl_in['layer_head_mask'] = None\n",
    "pt_in['head_mask'] = None\n",
    "\n",
    "fl_in['attention_mask'] = fl_inputs['attention_mask']\n",
    "pt_in['attention_mask'] = torch.Tensor(pt_inputs['attention_mask'].numpy())\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in, debug=True)\n",
    "att_in = fl_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense.weight torch.Size([3072, 768])\n",
      "dense.bias torch.Size([3072])\n",
      "Num Prams:  2362368\n",
      "+---------------------+-------------+-----------+----------+--------+\n",
      "| Name                | Shape       | Size      | Mean     | Std    |\n",
      "+---------------------+-------------+-----------+----------+--------+\n",
      "| params/dense/bias   | (3072,)     | 3,072     | 0.000241 | 0.0206 |\n",
      "| params/dense/kernel | (768, 3072) | 2,359,296 | 1.83e-05 | 0.0208 |\n",
      "+---------------------+-------------+-----------+----------+--------+\n",
      "Total: 2,362,368\n",
      "Forward diff:  0.13115513\n",
      "Flax relprop: [[[3.8992954e-04 5.3653643e-05 4.7216576e-04 ... 4.1869929e-04\n",
      "   7.1022027e-05 3.6419058e-04]\n",
      "  [2.4147255e-04 2.8888331e-04 1.6696571e-04 ... 3.4822416e-04\n",
      "   2.8289788e-04 2.4933330e-04]\n",
      "  [5.3986994e-04 3.0098023e-04 2.2024555e-04 ... 4.5775305e-05\n",
      "   2.6327156e-04 5.9411264e-05]\n",
      "  [2.0027079e-04 3.4479052e-04 4.6521294e-04 ... 3.4470871e-04\n",
      "   2.4357894e-04 3.8454970e-04]\n",
      "  [1.7698202e-04 3.1175360e-04 1.9104019e-04 ... 1.4931250e-05\n",
      "   3.0316706e-04 4.1728426e-04]]]\n",
      "Pt relprop: tensor([[[3.8993e-04, 5.3654e-05, 4.7217e-04,  ..., 4.1870e-04,\n",
      "          7.1022e-05, 3.6419e-04],\n",
      "         [2.4147e-04, 2.8888e-04, 1.6697e-04,  ..., 3.4822e-04,\n",
      "          2.8290e-04, 2.4933e-04],\n",
      "         [5.3987e-04, 3.0098e-04, 2.2025e-04,  ..., 4.5775e-05,\n",
      "          2.6327e-04, 5.9411e-05],\n",
      "         [2.0027e-04, 3.4479e-04, 4.6521e-04,  ..., 3.4471e-04,\n",
      "          2.4358e-04, 3.8455e-04],\n",
      "         [1.7698e-04, 3.1175e-04, 1.9104e-04,  ..., 1.4931e-05,\n",
      "          3.0317e-04, 4.1728e-04]]], grad_fn=<SubBackward0>)\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop diff: 6.323046e-08\n",
      "Cam sum: tensor(1., grad_fn=<SumBackward0>) 1.0\n",
      "torch.Size([1, 5, 3072])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Verified\"\"\"\n",
    "# <- BertAttention\n",
    "pt_m = BertIntermediate(configuration).eval()\n",
    "fl_m = FlaxBertIntermediate(configuration)\n",
    "# inputs = np.random.rand(*fl_out.shape)\n",
    "att_out = np.random.rand(*fl_out.shape)\n",
    "\n",
    "fl_in = {'hidden_states': jnp.array(att_out)}\n",
    "pt_in = {'hidden_states': torch.Tensor(att_out)}\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in, debug=True)\n",
    "print(pt_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 3072)\n",
      "(1, 5, 768)\n",
      "dense.weight torch.Size([768, 3072])\n",
      "dense.bias torch.Size([768])\n",
      "LayerNorm.weight torch.Size([768])\n",
      "LayerNorm.bias torch.Size([768])\n",
      "Num Prams:  2361600\n",
      "+------------------------+-------------+-----------+-----------+--------+\n",
      "| Name                   | Shape       | Size      | Mean      | Std    |\n",
      "+------------------------+-------------+-----------+-----------+--------+\n",
      "| params/LayerNorm/bias  | (768,)      | 768       | 0.0       | 0.0    |\n",
      "| params/LayerNorm/scale | (768,)      | 768       | 1.0       | 0.0    |\n",
      "| params/dense/bias      | (768,)      | 768       | -4.52e-05 | 0.0106 |\n",
      "| params/dense/kernel    | (3072, 768) | 2,359,296 | 2.92e-06  | 0.0104 |\n",
      "+------------------------+-------------+-----------+-----------+--------+\n",
      "Total: 2,361,600\n",
      "Forward diff:  0.0008989207\n",
      "Flax relprop: (DeviceArray([[[ 4.27302148e-05,  5.57678231e-06,  2.51676993e-05, ...,\n",
      "                5.88701914e-06,  2.01909188e-06, -1.92677305e-07],\n",
      "              [-8.17764976e-06, -1.00507032e-05,  1.07308358e-06, ...,\n",
      "               -3.72021386e-05, -3.52068055e-05, -5.39558678e-05],\n",
      "              [-9.80125405e-05,  1.42311677e-03,  3.64394218e-04, ...,\n",
      "               -2.39029632e-05, -3.48632602e-06,  1.46146223e-03],\n",
      "              [ 5.39080975e-05,  3.15343859e-05,  5.44711147e-05, ...,\n",
      "                6.54623145e-05, -1.58139283e-05, -3.99985656e-05],\n",
      "              [ 3.41791008e-07,  1.10537068e-04,  6.80750918e-06, ...,\n",
      "               -1.11935515e-05,  6.06187041e-06,  3.37998176e-06]]],            dtype=float32), DeviceArray([[[ 1.62458873e-05,  6.66025662e-05,  3.66133690e-06, ...,\n",
      "                5.09280071e-04,  1.36308791e-03,  1.33163587e-04],\n",
      "              [ 7.00886885e-04,  1.24453451e-04,  1.14177674e-04, ...,\n",
      "                7.30572327e-04,  6.20895007e-04,  2.08411351e-04],\n",
      "              [ 2.30467354e-04,  5.53405735e-05,  1.99369970e-04, ...,\n",
      "                6.49085341e-05, -4.92253050e-04,  2.09262027e-04],\n",
      "              [ 5.22458786e-03,  6.50338188e-04,  4.57646820e-04, ...,\n",
      "                3.50426504e-04,  4.66371421e-04,  3.71777103e-04],\n",
      "              [ 2.45724426e-04,  1.05459396e-04,  9.50490066e-05, ...,\n",
      "                2.46326148e-04,  4.93101106e-05,  2.68669886e-04]]],            dtype=float32))\n",
      "Pt relprop: (tensor([[[ 4.2730e-05,  5.5768e-06,  2.5168e-05,  ...,  5.8870e-06,\n",
      "           2.0191e-06, -1.9267e-07],\n",
      "         [-8.1776e-06, -1.0050e-05,  1.0731e-06,  ..., -3.7197e-05,\n",
      "          -3.5208e-05, -5.3955e-05],\n",
      "         [-9.8013e-05,  1.4215e-03,  3.6400e-04,  ..., -2.3903e-05,\n",
      "          -3.4863e-06,  1.4599e-03],\n",
      "         [ 5.3911e-05,  3.1536e-05,  5.4476e-05,  ...,  6.5464e-05,\n",
      "          -1.5812e-05, -3.9998e-05],\n",
      "         [ 3.4179e-07,  1.1054e-04,  6.8074e-06,  ..., -1.1194e-05,\n",
      "           6.0619e-06,  3.3800e-06]]], grad_fn=<SubBackward0>), tensor([[[ 1.6246e-05,  6.6603e-05,  3.6613e-06,  ...,  5.0928e-04,\n",
      "           1.3631e-03,  1.3316e-04],\n",
      "         [ 7.0089e-04,  1.2445e-04,  1.1418e-04,  ...,  7.3057e-04,\n",
      "           6.2090e-04,  2.0841e-04],\n",
      "         [ 2.3047e-04,  5.5341e-05,  1.9937e-04,  ...,  6.4909e-05,\n",
      "          -4.9225e-04,  2.0926e-04],\n",
      "         [ 5.2246e-03,  6.5034e-04,  4.5765e-04,  ...,  3.5043e-04,\n",
      "           4.6637e-04,  3.7178e-04],\n",
      "         [ 2.4572e-04,  1.0546e-04,  9.5049e-05,  ...,  2.4633e-04,\n",
      "           4.9310e-05,  2.6867e-04]]], grad_fn=<MulBackward0>))\n",
      "Relprop size: torch.Size([1, 5, 3072])\n",
      "Relprop 0  diff: 0.0009463803\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop 1  diff: 0.00095120614\n",
      "Cam sum: tensor(1., grad_fn=<AddBackward0>) 0.9999999\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Verified??\"\"\"\n",
    "#TODO: make sure it's verified\n",
    "pt_m = BertOutput(configuration).eval()\n",
    "fl_m = FlaxBertOutput(configuration)\n",
    "# inputs = np.random.rand(*fl_out.shape)\n",
    "# att_out = np.random.rand(*att_out.shape)\n",
    "inputs = np.random.rand(1,5,3072)\n",
    "att_out = np.random.rand(1,5,768)\n",
    "print(inputs.shape)\n",
    "print(att_out.shape)\n",
    "\n",
    "fl_in = {'hidden_states': jnp.array(inputs)}\n",
    "pt_in = {'hidden_states': torch.Tensor(inputs)}\n",
    "\n",
    "# print(np.sum(np.array(fl_in['hidden_states']) - pt_in['hidden_states'].numpy()))\n",
    "# fl_in['layer_head_mask'] = None\n",
    "# pt_in['head_mask'] = None\n",
    "\n",
    "fl_in['attention_output'] = jnp.array(att_out)\n",
    "pt_in['input_tensor'] = torch.Tensor(att_out)\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in, debug=True)\n",
    "print(pt_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.weight torch.Size([768, 768])\n",
      "query.bias torch.Size([768])\n",
      "key.weight torch.Size([768, 768])\n",
      "key.bias torch.Size([768])\n",
      "value.weight torch.Size([768, 768])\n",
      "value.bias torch.Size([768])\n",
      "Num Prams:  1771776\n",
      "+---------------------+------------+---------+-----------+--------+\n",
      "| Name                | Shape      | Size    | Mean      | Std    |\n",
      "+---------------------+------------+---------+-----------+--------+\n",
      "| params/key/bias     | (768,)     | 768     | 0.000462  | 0.021  |\n",
      "| params/key/kernel   | (768, 768) | 589,824 | 1.45e-05  | 0.0208 |\n",
      "| params/query/bias   | (768,)     | 768     | 0.000315  | 0.0212 |\n",
      "| params/query/kernel | (768, 768) | 589,824 | 1.38e-05  | 0.0208 |\n",
      "| params/value/bias   | (768,)     | 768     | -0.000179 | 0.0213 |\n",
      "| params/value/kernel | (768, 768) | 589,824 | 1.19e-05  | 0.0208 |\n",
      "+---------------------+------------+---------+-----------+--------+\n",
      "Total: 1,771,776\n",
      "Forward diff:  0.00014540557\n",
      "Flax relprop: [[[7.0450566e-05 1.2004062e-04 3.9115133e-05 ... 1.1220657e-04\n",
      "   2.1023920e-04 8.7152308e-05]\n",
      "  [3.2381844e-04 6.0843857e-05 3.7037030e-06 ... 2.5665631e-05\n",
      "   1.2515792e-04 1.8271641e-04]\n",
      "  [4.8320417e-06 1.4133643e-05 4.4966364e-05 ... 3.2867555e-04\n",
      "   1.3226058e-06 2.1688122e-04]\n",
      "  [3.1260428e-05 1.1045791e-04 7.0905400e-05 ... 1.5210046e-04\n",
      "   6.6868954e-05 1.5065393e-04]\n",
      "  [2.3282027e-04 2.8018744e-04 5.8388097e-05 ... 1.8948880e-04\n",
      "   2.5633682e-04 1.3192929e-05]]]\n",
      "Pt relprop: tensor([[[7.0450e-05, 1.2004e-04, 3.9113e-05,  ..., 1.1220e-04,\n",
      "          2.1022e-04, 8.7152e-05],\n",
      "         [3.2384e-04, 6.0844e-05, 3.7018e-06,  ..., 2.5666e-05,\n",
      "          1.2517e-04, 1.8272e-04],\n",
      "         [4.8266e-06, 1.4132e-05, 4.4964e-05,  ..., 3.2870e-04,\n",
      "          1.3222e-06, 2.1688e-04],\n",
      "         [3.1262e-05, 1.1046e-04, 7.0907e-05,  ..., 1.5211e-04,\n",
      "          6.6873e-05, 1.5065e-04],\n",
      "         [2.3282e-04, 2.8019e-04, 5.8390e-05,  ..., 1.8947e-04,\n",
      "          2.5633e-04, 1.3193e-05]]], grad_fn=<MulBackward0>)\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop diff: 2.3292574e-05\n",
      "Cam sum: tensor(0.5056, grad_fn=<SumBackward0>) 0.5056263\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "#TODO: not verified\n",
    "pt_m = BertSelfAttention(configuration).eval()\n",
    "fl_m = FlaxBertSelfAttention(configuration)\n",
    "inputs = np.random.rand(*fl_out.shape)\n",
    "fl_in = {'hidden_states': jnp.array(inputs)}\n",
    "pt_in = {'hidden_states': torch.Tensor(inputs)}\n",
    "\n",
    "# print(np.sum(np.array(fl_in['hidden_states']) - pt_in['hidden_states'].numpy()))\n",
    "fl_in['layer_head_mask'] = None\n",
    "pt_in['head_mask'] = None\n",
    "\n",
    "fl_in['attention_mask'] = fl_inputs['attention_mask']\n",
    "pt_in['attention_mask'] = torch.Tensor(pt_inputs['attention_mask'].numpy())\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in, debug=True)\n",
    "print(pt_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense.weight torch.Size([768, 768])\n",
      "dense.bias torch.Size([768])\n",
      "LayerNorm.weight torch.Size([768])\n",
      "LayerNorm.bias torch.Size([768])\n",
      "Num Prams:  592128\n",
      "+------------------------+------------+---------+----------+--------+\n",
      "| Name                   | Shape      | Size    | Mean     | Std    |\n",
      "+------------------------+------------+---------+----------+--------+\n",
      "| params/LayerNorm/bias  | (768,)     | 768     | 0.0      | 0.0    |\n",
      "| params/LayerNorm/scale | (768,)     | 768     | 1.0      | 0.0    |\n",
      "| params/dense/bias      | (768,)     | 768     | 0.000351 | 0.0213 |\n",
      "| params/dense/kernel    | (768, 768) | 589,824 | 2.23e-05 | 0.0208 |\n",
      "+------------------------+------------+---------+----------+--------+\n",
      "Total: 592,128\n",
      "Forward diff:  0.0008256881\n",
      "Flax relprop: (DeviceArray([[[ 2.82236997e-05,  8.73943864e-05, -2.32904658e-05, ...,\n",
      "               -3.73112634e-05,  3.64667030e-05,  5.59769148e-07],\n",
      "              [ 2.14660453e-04,  1.70848158e-04, -1.83097359e-06, ...,\n",
      "               -1.22228648e-05, -4.12660938e-06,  5.41310583e-04],\n",
      "              [-9.08234654e-07,  1.53865949e-05,  7.02543912e-06, ...,\n",
      "                4.27012492e-05, -2.78666284e-05,  2.53962007e-05],\n",
      "              [-3.11042822e-05, -3.62552455e-05,  5.41783174e-06, ...,\n",
      "                1.24075764e-03, -1.00080274e-06,  1.01979554e-03],\n",
      "              [ 3.01586206e-05,  3.26142726e-05,  4.82598334e-05, ...,\n",
      "                2.44775820e-05,  1.19612414e-04,  1.23771883e-06]]],            dtype=float32), DeviceArray([[[ 3.6742088e-06,  5.3765300e-05, -1.4874476e-04, ...,\n",
      "               -4.4252447e-05, -9.0410776e-06,  1.4006160e-04],\n",
      "              [ 1.3996266e-04,  2.0424722e-04, -7.0140450e-05, ...,\n",
      "               -3.0014117e-03,  3.5779609e-04, -4.9117644e-04],\n",
      "              [ 9.9604204e-06,  1.0715022e-04,  1.4816380e-04, ...,\n",
      "               -1.2671999e-06, -3.7196227e-05, -3.4243646e-06],\n",
      "              [ 9.5828356e-05,  1.2229283e-04,  2.1883572e-04, ...,\n",
      "               -2.6242521e-06, -6.7007713e-05,  3.3639203e-04],\n",
      "              [ 1.5540981e-04, -7.4036339e-05, -8.4245676e-04, ...,\n",
      "               -2.5516672e-05, -8.1081875e-05, -4.0239145e-05]]],            dtype=float32))\n",
      "Pt relprop: (tensor([[[ 2.8203e-05,  8.7330e-05, -2.3273e-05,  ..., -3.7284e-05,\n",
      "           3.6440e-05,  5.5958e-07],\n",
      "         [ 2.1451e-04,  1.7073e-04, -1.8296e-06,  ..., -1.2214e-05,\n",
      "          -4.1235e-06,  5.4092e-04],\n",
      "         [-9.0756e-07,  1.5375e-05,  7.0202e-06,  ...,  4.2669e-05,\n",
      "          -2.7846e-05,  2.5377e-05],\n",
      "         [-3.1081e-05, -3.6228e-05,  5.4137e-06,  ...,  1.2416e-03,\n",
      "          -1.0001e-06,  1.0191e-03],\n",
      "         [ 3.0137e-05,  3.2590e-05,  4.8223e-05,  ...,  2.4460e-05,\n",
      "           1.1952e-04,  1.2373e-06]]], grad_fn=<SubBackward0>), tensor([[[ 3.6715e-06,  5.3725e-05, -1.4863e-04,  ..., -4.4219e-05,\n",
      "          -9.0343e-06,  1.3996e-04],\n",
      "         [ 1.3986e-04,  2.0410e-04, -7.0088e-05,  ..., -2.9991e-03,\n",
      "           3.5753e-04, -4.9081e-04],\n",
      "         [ 9.9530e-06,  1.0707e-04,  1.4805e-04,  ..., -1.2663e-06,\n",
      "          -3.7169e-05, -3.4218e-06],\n",
      "         [ 9.5757e-05,  1.2220e-04,  2.1867e-04,  ..., -2.6223e-06,\n",
      "          -6.6958e-05,  3.3614e-04],\n",
      "         [ 1.5529e-04, -7.3981e-05, -8.4183e-04,  ..., -2.5498e-05,\n",
      "          -8.1021e-05, -4.0209e-05]]], grad_fn=<MulBackward0>))\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop 0  diff: 0.00041084574\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop 1  diff: 0.00094335596\n",
      "Cam sum: tensor(1., grad_fn=<AddBackward0>) 1.0\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Verified?\"\"\"\n",
    "pt_m = BertSelfOutput(configuration).eval()\n",
    "fl_m = FlaxBertSelfOutput(configuration)\n",
    "inputs = np.random.rand(*fl_in['hidden_states'].shape)\n",
    "att_out = np.random.rand(*fl_out.shape)\n",
    "\n",
    "fl_in = {'hidden_states': jnp.array(inputs)}\n",
    "pt_in = {'hidden_states': torch.Tensor(inputs)}\n",
    "\n",
    "# print(np.sum(np.array(fl_in['hidden_states']) - pt_in['hidden_states'].numpy()))\n",
    "# fl_in['layer_head_mask'] = None\n",
    "# pt_in['head_mask'] = None\n",
    "\n",
    "fl_in['input_tensor'] = jnp.array(att_out)\n",
    "pt_in['input_tensor'] = torch.Tensor(att_out)\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in, debug=True)\n",
    "print(pt_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward diff:  0.11965071\n",
      "Flax relprop: [[[ 8.48653654e-05  1.01908773e-01  3.10889591e-05 ...  1.89764833e-04\n",
      "   -3.00654629e-03  1.10478613e-05]\n",
      "  [-2.88873067e-04  7.17080082e-04  3.53923388e-04 ...  4.01802245e-04\n",
      "    9.63633764e-04  3.97505268e-04]\n",
      "  [ 1.65687874e-04  4.61450400e-04  1.78729111e-04 ...  7.30826752e-04\n",
      "   -4.49524705e-05  1.17540796e-04]\n",
      "  [ 7.93454892e-05  2.75159575e-04  2.36273932e-04 ...  3.93499824e-04\n",
      "    3.14914156e-04  8.58562475e-04]\n",
      "  [ 1.93362255e-04  2.54858285e-04 -1.17845193e-04 ...  9.48987872e-06\n",
      "   -2.38654378e-04  3.08102433e-04]]]\n",
      "Pt relprop: tensor([[[ 8.1814e-05,  9.6743e-02,  3.1333e-05,  ...,  1.8202e-04,\n",
      "          -2.8534e-03,  1.1549e-05],\n",
      "         [-2.7374e-04,  6.8380e-04,  3.3770e-04,  ...,  3.8273e-04,\n",
      "           9.1681e-04,  3.8019e-04],\n",
      "         [ 1.5841e-04,  4.3890e-04,  1.7024e-04,  ...,  6.9430e-04,\n",
      "          -4.2650e-05,  1.1239e-04],\n",
      "         [ 7.9241e-05,  2.7455e-04,  2.2947e-04,  ...,  3.9703e-04,\n",
      "           3.1314e-04,  8.4683e-04],\n",
      "         [ 1.8929e-04,  2.5819e-04, -3.3976e-05,  ...,  2.2628e-05,\n",
      "          -2.0507e-04,  2.9400e-04]]], grad_fn=<MulBackward0>)\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop diff: 0.10173912\n",
      "Cam sum: tensor(0.9379, grad_fn=<SumBackward0>) 0.93848866\n"
     ]
    }
   ],
   "source": [
    "pt_m = BertLayer(configuration).eval()\n",
    "fl_m = FlaxBertLayer(configuration)\n",
    "inputs = np.random.rand(*fl_out.shape)\n",
    "fl_in = {'hidden_states': jnp.array(inputs)}\n",
    "pt_in = {'hidden_states': torch.Tensor(inputs)}\n",
    "\n",
    "fl_in['layer_head_mask'] = None\n",
    "pt_in['head_mask'] = None\n",
    "\n",
    "fl_in['attention_mask'] = fl_inputs['attention_mask']\n",
    "pt_in['attention_mask'] = torch.Tensor(pt_inputs['attention_mask'].numpy())\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward diff:  0.41589844\n",
      "Flax relprop: [[[ 9.17672587e-05  1.21322119e-04  1.43126192e-04 ...  4.42310702e-05\n",
      "    6.37698104e-05  3.98187221e-05]\n",
      "  [ 4.50247608e-05  3.28941860e-06  1.30123090e-05 ... -3.64978878e-05\n",
      "   -8.27399854e-05  1.49486286e-05]\n",
      "  [ 2.06595185e-04  1.63453260e-05  1.94815831e-04 ...  1.21141013e-04\n",
      "    8.82099084e-06  8.85548616e-06]\n",
      "  [ 1.54863548e-04  1.39264430e-05  1.18865428e-04 ...  1.25770594e-05\n",
      "    4.82141195e-06  4.93427524e-06]\n",
      "  [ 4.09272616e-05  3.83845281e-06 -1.18937234e-04 ...  2.40647223e-05\n",
      "    1.96528286e-04 -5.24184088e-06]]]\n",
      "Pt relprop: tensor([[[ 1.5006e-04,  2.4347e-04,  2.2321e-04,  ...,  7.4723e-05,\n",
      "           3.8389e-05,  7.7174e-05],\n",
      "         [ 9.0576e-05,  7.6950e-06,  2.5329e-05,  ..., -2.7093e-07,\n",
      "          -1.9552e-04,  2.7035e-05],\n",
      "         [ 3.6087e-04,  3.8663e-05,  3.3454e-04,  ...,  1.9583e-04,\n",
      "           4.3585e-05,  1.6265e-05],\n",
      "         [ 3.6521e-04,  3.5574e-05,  2.9966e-04,  ...,  3.3067e-05,\n",
      "           7.4273e-06,  1.5094e-05],\n",
      "         [ 1.0754e-04,  1.1451e-05, -1.1188e-04,  ...,  2.8977e-05,\n",
      "           1.0787e-04,  4.6654e-06]]], grad_fn=<MulBackward0>)\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop diff: 1.2966945\n",
      "Cam sum: tensor(0.5490, grad_fn=<SumBackward0>) 0.49327385\n"
     ]
    }
   ],
   "source": [
    "pt_m = BertEncoder(configuration).eval()\n",
    "fl_m = FlaxBertEncoder(configuration)\n",
    "\n",
    "# requires: hidden_states, attention_mask, head_mask (default None)\n",
    "inputs = np.random.rand(*fl_out.shape)\n",
    "fl_in = {'hidden_states': jnp.array(inputs)}\n",
    "pt_in = {'hidden_states': torch.Tensor(inputs)}\n",
    "\n",
    "fl_in['head_mask'] = None\n",
    "pt_in['head_mask'] = None\n",
    "\n",
    "fl_in['attention_mask'] = fl_inputs['attention_mask']\n",
    "pt_in['attention_mask'] = torch.Tensor(pt_inputs['attention_mask'].numpy())\n",
    "\n",
    "# print()\n",
    "\n",
    "\n",
    "# print(fl_in, pt_in)\n",
    "# fl_in.pop('attention_mask', None)\n",
    "# pt_in.pop('attention_mask', None)\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fl_in = (input_ids, token_ids, position_ids, attention_mask)\n",
    "# pt_in = (pt_input_ids, pt_token_ids, pt_position_ids, None)\n",
    "# fl_in = fl_inputs\n",
    "# pt_in = pt_inputs\n",
    "# pt_out, fl_out = verify_module(pt_layer, fl_layer, pt_in, fl_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward diff:  0.06005927\n",
      "tensor([[[0.0003, 0.0023, 0.0011,  ..., 0.0016, 0.0009, 0.0032],\n",
      "         [-0.0000, -0.0000, -0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, -0.0000, -0.0000,  ..., 0.0000, -0.0000, 0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[[ 9.4230e-04,  1.3651e-04,  3.1703e-04,  ...,  1.9178e-04,\n",
      "           6.6972e-04, -1.6566e-04],\n",
      "         [-2.3229e-05,  2.5664e-04,  2.8254e-04,  ...,  1.7318e-04,\n",
      "           1.7434e-08, -7.2397e-05],\n",
      "         [-5.8297e-06,  2.2902e-04,  2.1100e-04,  ...,  2.1256e-04,\n",
      "           5.1631e-05,  3.5355e-04],\n",
      "         [ 1.8829e-03, -4.0353e-05,  1.0227e-04,  ..., -3.8071e-04,\n",
      "          -2.0600e-05,  1.6200e-03],\n",
      "         [ 5.9504e-05,  9.0210e-05,  2.3783e-04,  ...,  2.5857e-04,\n",
      "           7.2307e-05,  3.0514e-04]]], grad_fn=<MulBackward0>)\n",
      "Flax relprop: [[[ 8.4658951e-04  3.9540220e-04  7.0065074e-04 ...  2.6467489e-04\n",
      "    1.7745949e-03  2.0642248e-03]\n",
      "  [ 1.2144186e-06  1.5832399e-04  3.0850995e-05 ...  2.6666556e-05\n",
      "    6.9393386e-06  6.0909137e-05]\n",
      "  [ 1.2824971e-06  3.6331778e-05  2.6886832e-04 ...  4.0559261e-04\n",
      "   -1.5061632e-05  2.9325866e-04]\n",
      "  [-1.6834631e-04 -1.7029420e-04 -4.2265622e-04 ...  7.8399506e-05\n",
      "    1.5764634e-05  2.2715225e-04]\n",
      "  [ 2.0428464e-05  1.3156504e-04  7.5812313e-05 ...  1.7666331e-04\n",
      "   -1.0036598e-05  2.0398166e-04]]]\n",
      "Pt relprop: tensor([[[ 9.4230e-04,  1.3651e-04,  3.1703e-04,  ...,  1.9178e-04,\n",
      "           6.6972e-04, -1.6566e-04],\n",
      "         [-2.3229e-05,  2.5664e-04,  2.8254e-04,  ...,  1.7318e-04,\n",
      "           1.7434e-08, -7.2397e-05],\n",
      "         [-5.8297e-06,  2.2902e-04,  2.1100e-04,  ...,  2.1256e-04,\n",
      "           5.1631e-05,  3.5355e-04],\n",
      "         [ 1.8829e-03, -4.0353e-05,  1.0227e-04,  ..., -3.8071e-04,\n",
      "          -2.0600e-05,  1.6200e-03],\n",
      "         [ 5.9504e-05,  9.0210e-05,  2.3783e-04,  ...,  2.5857e-04,\n",
      "           7.2307e-05,  3.0514e-04]]], grad_fn=<MulBackward0>)\n",
      "Relprop size: torch.Size([1, 5, 768])\n",
      "Relprop diff: 1.6730683\n",
      "Cam sum: tensor(1.0000, grad_fn=<SumBackward0>) 1.0000002\n"
     ]
    }
   ],
   "source": [
    "pt_m = BertModel(configuration).eval()\n",
    "fl_m = FlaxBertModule(configuration)\n",
    "fl_in = fl_inputs\n",
    "pt_in = pt_inputs\n",
    "pt_out, fl_out = verify_module(pt_m, fl_m, pt_in, fl_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8198765870>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.abs(fl_out.flatten()))\n",
    "plt.plot(np.abs(pt_out.detach().numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parameter_overview.get_parameter_overview())\n",
    "print(pt_layer, count_parameters(pt_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in pt_layer.named_parameters():\n",
    "    print(k, v.shape)\n",
    "\n",
    "vf = {'params':\n",
    "          {'word_embeddings':\n",
    "               {'embedding': None},\n",
    "           'position_embeddings':\n",
    "               {'embedding': None},\n",
    "           'token_type_embeddings':\n",
    "               {'embedding': None},\n",
    "           'LayerNorm':\n",
    "               {'bias': None,\n",
    "                'scale': None},\n",
    "        }\n",
    "      }\n",
    "for k,v in pt_layer.named_parameters():\n",
    "    if k == 'word_embeddings.weight':\n",
    "        vf['params']['word_embeddings']['embedding'] = v.detach().numpy()\n",
    "    if k == 'position_embeddings.weight':\n",
    "        vf['params']['position_embeddings']['embedding'] = v.detach().numpy()\n",
    "    if k == 'token_type_embeddings.weight':\n",
    "        vf['params']['token_type_embeddings']['embedding'] = v.detach().numpy()\n",
    "    if k == 'LayerNorm.weight':\n",
    "        vf['params']['LayerNorm']['scale'] = v.detach().numpy()\n",
    "    if k == 'LayerNorm.bias':\n",
    "        vf['params']['LayerNorm']['bias'] = v.detach().numpy()\n",
    "\n",
    "print(vf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vf = f_layer.init(jax.random.PRNGKey(0), input_ids, token_ids,  position_ids, attention_mask)\n",
    "f_out = f_layer.apply(vf, input_ids, token_ids, position_ids, attention_mask)\n",
    "pt_out = pt_layer(input_ids=pt_input_ids, token_type_ids=pt_token_ids, position_ids=pt_position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30522, 768)\n",
      "(1, 5, 768)\n",
      "torch.Size([1, 5, 768])\n",
      "3.368198e-05\n"
     ]
    }
   ],
   "source": [
    "print(vf['params']['word_embeddings']['embedding'].shape)\n",
    "print(f_out.shape)\n",
    "print(pt_out.shape)\n",
    "print(np.abs(pt_out.detach().numpy() - f_out).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relprop 2\n",
      "2.7899752\n",
      "2.787298\n",
      "2.7194073\n",
      "1.0000002\n",
      "1.0\n",
      "(1, 5, 768)\n"
     ]
    }
   ],
   "source": [
    "cam = np.random.rand(*f_out.shape)\n",
    "f_cam = jnp.array( cam / cam.sum())\n",
    "pt_cam = torch.tensor(cam / cam.sum())\n",
    "\n",
    "f_cam1, f_cam2 = f_layer.apply(vf, f_cam, input_ids, token_ids, position_ids, attention_mask, method=f_layer.relprop)\n",
    "kwargs = {'alpha': 1}\n",
    "pt_cam1, pt_cam2 = pt_layer.relprop(pt_cam, **kwargs)\n",
    "\n",
    "\n",
    "print(np.abs(np.array(f_cam1) - pt_cam1.detach().numpy()).sum())\n",
    "print(np.abs(np.array(f_cam2) - pt_cam2.detach().numpy()).sum())\n",
    "print(np.abs(np.array(f_cam1) - pt_cam2.detach().numpy()).sum())\n",
    "print(np.sum(f_cam1) + np.sum(f_cam2))\n",
    "print(np.sum(pt_cam1.detach().numpy()) + np.sum(pt_cam2.detach().numpy()))\n",
    "print(f_cam2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+--------------+------------+-----------+-------+\n",
      "| Name                                   | Shape        | Size       | Mean      | Std   |\n",
      "+----------------------------------------+--------------+------------+-----------+-------+\n",
      "| params/LayerNorm/bias                  | (768,)       | 768        | 0.0       | 0.0   |\n",
      "| params/LayerNorm/scale                 | (768,)       | 768        | 1.0       | 0.0   |\n",
      "| params/position_embeddings/embedding   | (512, 768)   | 393,216    | -0.00286  | 0.999 |\n",
      "| params/token_type_embeddings/embedding | (2, 768)     | 1,536      | -0.00341  | 1.01  |\n",
      "| params/word_embeddings/embedding       | (30522, 768) | 23,440,896 | -7.33e-05 | 1.0   |\n",
      "+----------------------------------------+--------------+------------+-----------+-------+\n",
      "Total: 23,837,184\n",
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (add1): Add()\n",
      "  (add2): Add()\n",
      ") 23837184\n"
     ]
    }
   ],
   "source": [
    "print(parameter_overview.get_parameter_overview(vf))\n",
    "print(pt_layer, count_parameters(pt_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_layer = BertEncoder(configuration).eval()\n",
    "f_layer = FlaxBertEncoder(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.0.attention.self.query.bias torch.Size([768])\n",
      "layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.0.attention.self.key.bias torch.Size([768])\n",
      "layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.0.attention.self.value.bias torch.Size([768])\n",
      "layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.0.attention.output.dense.bias torch.Size([768])\n",
      "layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "layer.0.output.dense.bias torch.Size([768])\n",
      "layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.1.attention.self.query.bias torch.Size([768])\n",
      "layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.1.attention.self.key.bias torch.Size([768])\n",
      "layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.1.attention.self.value.bias torch.Size([768])\n",
      "layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.1.attention.output.dense.bias torch.Size([768])\n",
      "layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "layer.1.output.dense.bias torch.Size([768])\n",
      "layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.2.attention.self.query.bias torch.Size([768])\n",
      "layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.2.attention.self.key.bias torch.Size([768])\n",
      "layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.2.attention.self.value.bias torch.Size([768])\n",
      "layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.2.attention.output.dense.bias torch.Size([768])\n",
      "layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "layer.2.output.dense.bias torch.Size([768])\n",
      "layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.3.attention.self.query.bias torch.Size([768])\n",
      "layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.3.attention.self.key.bias torch.Size([768])\n",
      "layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.3.attention.self.value.bias torch.Size([768])\n",
      "layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.3.attention.output.dense.bias torch.Size([768])\n",
      "layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "layer.3.output.dense.bias torch.Size([768])\n",
      "layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.4.attention.self.query.bias torch.Size([768])\n",
      "layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.4.attention.self.key.bias torch.Size([768])\n",
      "layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.4.attention.self.value.bias torch.Size([768])\n",
      "layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.4.attention.output.dense.bias torch.Size([768])\n",
      "layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "layer.4.output.dense.bias torch.Size([768])\n",
      "layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.5.attention.self.query.bias torch.Size([768])\n",
      "layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.5.attention.self.key.bias torch.Size([768])\n",
      "layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.5.attention.self.value.bias torch.Size([768])\n",
      "layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.5.attention.output.dense.bias torch.Size([768])\n",
      "layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "layer.5.output.dense.bias torch.Size([768])\n",
      "layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.6.attention.self.query.bias torch.Size([768])\n",
      "layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.6.attention.self.key.bias torch.Size([768])\n",
      "layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.6.attention.self.value.bias torch.Size([768])\n",
      "layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.6.attention.output.dense.bias torch.Size([768])\n",
      "layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "layer.6.output.dense.bias torch.Size([768])\n",
      "layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.7.attention.self.query.bias torch.Size([768])\n",
      "layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.7.attention.self.key.bias torch.Size([768])\n",
      "layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.7.attention.self.value.bias torch.Size([768])\n",
      "layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.7.attention.output.dense.bias torch.Size([768])\n",
      "layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "layer.7.output.dense.bias torch.Size([768])\n",
      "layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.8.attention.self.query.bias torch.Size([768])\n",
      "layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.8.attention.self.key.bias torch.Size([768])\n",
      "layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.8.attention.self.value.bias torch.Size([768])\n",
      "layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.8.attention.output.dense.bias torch.Size([768])\n",
      "layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "layer.8.output.dense.bias torch.Size([768])\n",
      "layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.9.attention.self.query.bias torch.Size([768])\n",
      "layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.9.attention.self.key.bias torch.Size([768])\n",
      "layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.9.attention.self.value.bias torch.Size([768])\n",
      "layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.9.attention.output.dense.bias torch.Size([768])\n",
      "layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "layer.9.output.dense.bias torch.Size([768])\n",
      "layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.10.attention.self.query.bias torch.Size([768])\n",
      "layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.10.attention.self.key.bias torch.Size([768])\n",
      "layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.10.attention.self.value.bias torch.Size([768])\n",
      "layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.10.attention.output.dense.bias torch.Size([768])\n",
      "layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "layer.10.output.dense.bias torch.Size([768])\n",
      "layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "layer.11.attention.self.query.bias torch.Size([768])\n",
      "layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "layer.11.attention.self.key.bias torch.Size([768])\n",
      "layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "layer.11.attention.self.value.bias torch.Size([768])\n",
      "layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "layer.11.attention.output.dense.bias torch.Size([768])\n",
      "layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "layer.11.output.dense.bias torch.Size([768])\n",
      "layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "layer.11.output.LayerNorm.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for k,v in pt_layer.named_parameters():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_m = BertModel(configuration).eval()\n",
    "pt_in = pt_inputs\n",
    "pt_out = pt_m(**pt_in)\n",
    "pt_out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/.conda/envs/dl_sys/lib/python3.10/site-packages/transformers/modeling_utils.py:735: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pt_out.sum().backward()\n",
    "one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "one_hot[0, index] = 1\n",
    "one_hot_vector = one_hot\n",
    "one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "one_hot = torch.sum(one_hot.cuda() * output)\n",
    "\n",
    "self.model.zero_grad()\n",
    "one_hot.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0026, 0.0008, 0.0001,  ..., 0.0013, 0.0007, 0.0017],\n",
      "         [-0.0000, 0.0000, 0.0000,  ..., 0.0000, -0.0000, -0.0000],\n",
      "         [-0.0000, 0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.0000, 0.0000, -0.0000,  ..., 0.0000, -0.0000, 0.0000],\n",
      "         [-0.0000, 0.0000, 0.0000,  ..., 0.0000, -0.0000, 0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[[ 1.4184e-03, -1.0720e-03,  6.7770e-04,  ...,  1.6326e-04,\n",
      "           2.1099e-03,  2.7981e-03],\n",
      "         [-1.3088e-04,  5.7206e-03,  9.1051e-04,  ..., -4.8278e-04,\n",
      "          -3.3931e-03,  2.3779e-03],\n",
      "         [ 1.9464e-04,  5.5653e-05, -1.4214e-05,  ...,  3.1613e-04,\n",
      "           4.0099e-05,  1.2043e-05],\n",
      "         [ 1.9841e-04,  6.3528e-04, -4.8302e-05,  ...,  6.4033e-06,\n",
      "          -1.9505e-04, -3.1525e-04],\n",
      "         [-9.5246e-06,  5.2908e-04,  1.9418e-04,  ..., -2.2420e-05,\n",
      "           3.5546e-04,  9.9432e-04]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cam = np.random.rand(*pt_out.shape)\n",
    "pt_cam = torch.tensor(cam / cam.sum())\n",
    "kwargs = {'alpha': 1}\n",
    "pt_cams = pt_m.relprop(pt_cam, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rollout_attention(all_layer_matrices, start_layer=0):\n",
    "    # adding residual consideration- code adapted from https://github.com/samiraabnar/attention_flow\n",
    "    num_tokens = all_layer_matrices[0].shape[1]\n",
    "    batch_size = all_layer_matrices[0].shape[0]\n",
    "    eye = torch.eye(num_tokens).expand(batch_size, num_tokens, num_tokens).to(all_layer_matrices[0].device)\n",
    "    all_layer_matrices = [all_layer_matrices[i] + eye for i in range(len(all_layer_matrices))]\n",
    "    matrices_aug = [all_layer_matrices[i] / all_layer_matrices[i].sum(dim=-1, keepdim=True)\n",
    "                          for i in range(len(all_layer_matrices))]\n",
    "    joint_attention = matrices_aug[start_layer]\n",
    "    for i in range(start_layer+1, len(matrices_aug)):\n",
    "        joint_attention = matrices_aug[i].bmm(joint_attention)\n",
    "    return joint_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0025, 0.0025, 0.0029, 0.0029, 0.0029]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cams = []\n",
    "blocks = pt_m.encoder.layer\n",
    "for blk in blocks:\n",
    "    grad = blk.attention.self.get_attn_gradients()\n",
    "    cam = blk.attention.self.get_attn_cam()\n",
    "    cam = cam[0].reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "    grad = grad[0].reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "    cam = grad * cam\n",
    "    cam = cam.clamp(min=0).mean(dim=0)\n",
    "    cams.append(cam.unsqueeze(0))\n",
    "rollout = compute_rollout_attention(cams, start_layer=0)\n",
    "rollout[:, 0, 0] = rollout[:, 0].min()\n",
    "rollout[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlsys",
   "language": "python",
   "name": "dlsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
